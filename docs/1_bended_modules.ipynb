{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bended modules \n",
    "\n",
    "At the very base of the `torchbend` modules lies the `BendedModule`, a wrapper for `torch.nn.Module` that uses the `torchbend` improved tracer to provide a handy interface for weight bending, interpolation, and activation retrieval / and bending. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dissecting weights and activations.\n",
    "\n",
    "Let's see how to bend this simple module : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys; sys.path.append(\"..\")\n",
    "import torch, torch.nn as nn\n",
    "import torchbend as tb\n",
    "\n",
    "class Stanley(nn.Module):\n",
    "    def __init__(self, n_channels = 4):\n",
    "        super().__init__()\n",
    "        self.conv_modules = nn.Sequential(\n",
    "            nn.Conv1d(1, n_channels, 3), \n",
    "            nn.Conv1d(n_channels, 8, 3)\n",
    "        )\n",
    "        self.batch_norm = nn.BatchNorm1d(8)\n",
    "        self.nnlin = nn.Sigmoid()\n",
    "        self.n_channels = n_channels\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.conv_modules(x)\n",
    "        out = self.batch_norm(out)\n",
    "        out = self.nnlin(out)\n",
    "        return out\n",
    "\n",
    "    def forward_nobatch(self, x):\n",
    "        out = self.conv_modules(x)\n",
    "        out = self.nnlin(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "# instantiate \n",
    "module = Stanley()\n",
    "\n",
    "# wrap with Bended Module\n",
    "bended = tb.BendedModule(module)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`BendedModule` is one the main object of the `torchbend` library, that takes a `nn.Module` instance as its only argument. Before bending, let's analyse the weights our `Stanley` instance through the `BendedModule` wrapper : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name                   shape                  dtype                min        max        mean     stddev\n",
      "---------------------  ---------------------  -------------  ---------  ---------  ----------  ---------\n",
      "conv_modules.0.weight  torch.Size([4, 1, 3])  torch.float32  -0.48003    0.431333  -0.0684427  0.315939\n",
      "conv_modules.0.bias    torch.Size([4])        torch.float32  -0.478022  -0.334556  -0.427702   0.0656257\n",
      "conv_modules.1.weight  torch.Size([8, 4, 3])  torch.float32  -0.284376   0.281184  -0.0247442  0.159428\n",
      "conv_modules.1.bias    torch.Size([8])        torch.float32  -0.272464   0.142609  -0.088053   0.156509\n",
      "batch_norm.weight      torch.Size([8])        torch.float32   1          1          1          0\n",
      "batch_norm.bias        torch.Size([8])        torch.float32   0          0          0          0\n"
     ]
    }
   ],
   "source": [
    "weight_names = bended.weight_names\n",
    "weight_shapes = list(map(bended.weight_shape, weight_names))\n",
    "\n",
    "# print_weights also print weight information in a tabular way. \n",
    "# The out keyword may be used to export information as a .txt file\n",
    "bended.print_weights();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perfect, we have all the information we need for Stanley's weights. To retrieve activations, target methods needs to be traced first ; this can be done with the `trace` callback, that requires a given set of inputs for the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward method : \n",
      "--------------  -----------  -----------------------\n",
      "x               placeholder  torch.Size([4, 1, 128])\n",
      "conv_modules_0  call_module  torch.Size([4, 4, 126])\n",
      "conv_modules_1  call_module  torch.Size([4, 8, 124])\n",
      "batch_norm      call_module  torch.Size([4, 8, 124])\n",
      "nnlin           call_module  torch.Size([4, 8, 124])\n",
      "--------------  -----------  -----------------------\n",
      "forward_nobatch method : \n",
      "--------------  -----------  -----------------------\n",
      "x               placeholder  torch.Size([4, 1, 128])\n",
      "conv_modules_0  call_module  torch.Size([4, 4, 126])\n",
      "conv_modules_1  call_module  torch.Size([4, 8, 124])\n",
      "nnlin           call_module  torch.Size([4, 8, 124])\n",
      "--------------  -----------  -----------------------\n"
     ]
    }
   ],
   "source": [
    "# by default, trace traces the forward callback.\n",
    "x = torch.zeros(4, 1, 128)\n",
    "bended.trace(x=x)\n",
    "activation_names = bended.activation_names()\n",
    "activation_shapes = list(map(bended.activation_shape, activation_names))\n",
    "print('forward method : ')\n",
    "bended.print_activations();\n",
    "\n",
    "# as activations are callback dependent, method name may be\n",
    "# given to specify the target : \n",
    "from functools import partial\n",
    "\n",
    "fn = \"forward_nobatch\"\n",
    "bended.trace(fn, x=x)\n",
    "activation_names = bended.activation_names(fn)\n",
    "activation_shapes = list(map(partial(bended.activation_shape, fn=fn), activation_names))\n",
    "print(f'{fn} method : ')\n",
    "bended.print_activations(fn);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By tracing a given method, `BendedModule` actually decomposes the method as a [torch.fx.Graph](https://pytorch.org/docs/stable/fx.html#torch.fx.Graph), tracking all the operations applied to a given set of inputs. The graph of a function is detached from the value of the module's parameters, such as the union of a graph and a state dict is called a [torch.fx.GraphModule](https://pytorch.org/docs/stable/fx.html#torch.fx.GraphModule). Both can be retrieved directly from `BendedModule` : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph : \n",
      "opcode       name            target          args               kwargs\n",
      "-----------  --------------  --------------  -----------------  --------\n",
      "placeholder  x               x               ()                 {}\n",
      "call_module  conv_modules_0  conv_modules.0  (x,)               {}\n",
      "call_module  conv_modules_1  conv_modules.1  (conv_modules_0,)  {}\n",
      "call_module  batch_norm      batch_norm      (conv_modules_1,)  {}\n",
      "call_module  nnlin           nnlin           (batch_norm,)      {}\n",
      "output       output          output          (nnlin,)           {}\n",
      "\n",
      "Graph : \n",
      "opcode       name            target          args               kwargs\n",
      "-----------  --------------  --------------  -----------------  --------\n",
      "placeholder  x               x               ()                 {}\n",
      "call_module  conv_modules_0  conv_modules.0  (x,)               {}\n",
      "call_module  conv_modules_1  conv_modules.1  (conv_modules_0,)  {}\n",
      "call_module  nnlin           nnlin           (conv_modules_1,)  {}\n",
      "output       output          output          (nnlin,)           {}\n"
     ]
    }
   ],
   "source": [
    "fn = \"forward\"\n",
    "graph = bended.graph(fn)\n",
    "print('Graph : ')\n",
    "graph.print_tabular()\n",
    "fn = \"forward_nobatch\"\n",
    "graph_module = bended.graph_module(fn)\n",
    "print('\\nGraph : ')\n",
    "graph_module.graph.print_tabular()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specific activations can be retrieved as a `dict` object using the `get_activations` method : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'conv_modules_0': torch.Size([4, 4, 126]), 'nnlin': torch.Size([4, 8, 124])}\n"
     ]
    }
   ],
   "source": [
    "outs = bended.get_activations(\"conv_modules_0\", \"nnlin\",  x=x, fn=\"forward\")\n",
    "print({k: v.shape for k, v in outs.items()})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bending\n",
    "\n",
    "Here we will see how to bend specific weights and activations. We will here use the `tb.Mask` bending operation, that masks the target feature using a binary mask. Bending operations does not touch the original module and are not made in place, such that every bending operation can be reverted using the `reset` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bended keys : ['conv_modules.0.weight', 'conv_modules.1.weight', 'nnlin']\n",
      "original == bended : tensor(False)\n",
      "original param == bended param : tensor(True)\n",
      "original == reverted : tensor(True)\n"
     ]
    }
   ],
   "source": [
    "out = bended.forward(x)\n",
    "\n",
    "cb = tb.Mask(prob=0.4)\n",
    "# Bending keys are given as regexp, allowing to target several keys at the same time!\n",
    "bended.bend(cb, \"conv_modules.\\d.weight\", \"nnlin\")\n",
    "print(\"bended keys :\", bended.bended_keys())\n",
    "\n",
    "out_bended = bended.forward(x)\n",
    "print(\"original == bended :\", (out == out_bended).all())\n",
    "print(\"original param == bended param :\", (module.conv_modules[0].weight == bended.module.conv_modules[0].weight).all())\n",
    "\n",
    "# revert bending\n",
    "bended.reset()\n",
    "out_reverted = bended.forward(x)\n",
    "print(\"original == reverted :\", (out == out_reverted).all())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, the module has been correctly bended! Let's see in details how `BendingModule` bends the original module. The process can be summarized as follows : \n",
    "\n",
    "\n",
    "![bending process](img/bending.png \"Bending process\")\n",
    "\n",
    "Let's now compare item by item the effect of the bending process using several inner methods of `BendingModule` : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- original state dict :\n",
      " tensor([[[-0.2799,  0.4313,  0.2887]],\n",
      "\n",
      "        [[-0.3577,  0.3159,  0.1079]],\n",
      "\n",
      "        [[-0.0639,  0.1702, -0.4800]],\n",
      "\n",
      "        [[-0.3317, -0.3025, -0.3195]]])\n",
      "-- bended state dict: \n",
      " tensor([[[-0.2799,  0.0000,  0.2887]],\n",
      "\n",
      "        [[-0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "        [[-0.0639,  0.0000, -0.0000]],\n",
      "\n",
      "        [[-0.0000, -0.3025, -0.0000]]])\n",
      "-- original graph :\n",
      " graph():\n",
      "    %x : [num_users=1] = placeholder[target=x]\n",
      "    %conv_modules_0 : [num_users=1] = call_module[target=conv_modules.0](args = (%x,), kwargs = {})\n",
      "    %conv_modules_1 : [num_users=1] = call_module[target=conv_modules.1](args = (%conv_modules_0,), kwargs = {})\n",
      "    %batch_norm : [num_users=1] = call_module[target=batch_norm](args = (%conv_modules_1,), kwargs = {})\n",
      "    %nnlin : [num_users=1] = call_module[target=nnlin](args = (%batch_norm,), kwargs = {})\n",
      "    return nnlin\n",
      "-- bended graph: \n",
      " graph():\n",
      "    %x : [num_users=1] = placeholder[target=x]\n",
      "    %conv_modules_0 : [num_users=1] = call_module[target=conv_modules.0](args = (%x,), kwargs = {})\n",
      "    %conv_modules_1 : [num_users=1] = call_module[target=conv_modules.1](args = (%conv_modules_0,), kwargs = {})\n",
      "    %batch_norm : [num_users=1] = call_module[target=batch_norm](args = (%conv_modules_1,), kwargs = {})\n",
      "    %nnlin : [num_users=1] = call_module[target=nnlin](args = (%batch_norm,), kwargs = {})\n",
      "    %nnlin_bended : [num_users=1] = call_module[target=nnlin_callback](args = (%nnlin,), kwargs = {name: nnlin})\n",
      "    return nnlin_bended\n",
      "torch.Size([4, 8, 124])\n",
      "torch.Size([4, 8, 124])\n",
      "-- original activation :\n",
      " tensor([0.4556, 0.4407, 0.4915, 0.4655, 0.4045, 0.6255, 0.3642, 0.6400, 0.2347,\n",
      "        0.3890, 0.2881, 0.3887, 0.7042, 0.3049, 0.8244, 0.4344, 0.8079, 0.6318,\n",
      "        0.8519, 0.3077, 0.8306, 0.5216, 0.7399, 0.8357, 0.2555, 0.7801, 0.3595,\n",
      "        0.6609, 0.6523, 0.3054, 0.5720, 0.2678, 0.6525, 0.4723, 0.8001, 0.2093,\n",
      "        0.6896, 0.2263, 0.3533, 0.5964, 0.4786, 0.6867, 0.5239, 0.9019, 0.4235,\n",
      "        0.8400, 0.1627, 0.5867, 0.1690, 0.6218, 0.0987, 0.4549, 0.2315, 0.4785,\n",
      "        0.6926, 0.3872, 0.8321, 0.3527, 0.5873, 0.4762, 0.4215, 0.6376, 0.2568,\n",
      "        0.5197, 0.3012, 0.4098, 0.4143, 0.4182, 0.5762, 0.5366, 0.5873, 0.4815,\n",
      "        0.6109, 0.5802, 0.5796, 0.5240, 0.2852, 0.4834, 0.2866, 0.5649, 0.4989,\n",
      "        0.4522, 0.3658, 0.2862, 0.5960, 0.4387, 0.4177, 0.6753, 0.3946, 0.6392,\n",
      "        0.7951, 0.4887, 0.8828, 0.4652, 0.8396, 0.7828, 0.5837, 0.6436, 0.4884,\n",
      "        0.3332, 0.5202, 0.4238, 0.3205, 0.4992, 0.4903, 0.5490, 0.3810, 0.6540,\n",
      "        0.4007, 0.4652, 0.4756, 0.2050, 0.4057, 0.3253, 0.1571, 0.5407, 0.5456,\n",
      "        0.5891, 0.9219, 0.2473, 0.8483, 0.3517, 0.7024, 0.5600],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "-- bended activation: \n",
      " tensor([0.4556, 0.4407, 0.4915, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.2881, 0.3887, 0.0000, 0.0000, 0.8244, 0.0000, 0.0000, 0.0000,\n",
      "        0.8519, 0.0000, 0.0000, 0.5216, 0.7399, 0.8357, 0.0000, 0.0000, 0.3595,\n",
      "        0.0000, 0.6523, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.8001, 0.0000,\n",
      "        0.6896, 0.0000, 0.3533, 0.0000, 0.4786, 0.6867, 0.5239, 0.0000, 0.0000,\n",
      "        0.8400, 0.0000, 0.5867, 0.1690, 0.6218, 0.0000, 0.4549, 0.0000, 0.0000,\n",
      "        0.0000, 0.3872, 0.0000, 0.0000, 0.0000, 0.4762, 0.4215, 0.6376, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.4182, 0.0000, 0.5366, 0.5873, 0.0000,\n",
      "        0.0000, 0.5802, 0.0000, 0.5240, 0.0000, 0.4834, 0.2866, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.6753, 0.0000, 0.6392,\n",
      "        0.0000, 0.4887, 0.0000, 0.0000, 0.8396, 0.0000, 0.5837, 0.0000, 0.4884,\n",
      "        0.0000, 0.0000, 0.0000, 0.3205, 0.0000, 0.4903, 0.5490, 0.0000, 0.0000,\n",
      "        0.0000, 0.4652, 0.0000, 0.2050, 0.4057, 0.0000, 0.0000, 0.5407, 0.5456,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.5600],\n",
      "       grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "cb = tb.Mask(prob=0.4)\n",
    "bended.bend(cb, \"conv_modules.0.weight\", \"nnlin\")\n",
    "\n",
    "original_state_dict = bended.state_dict()\n",
    "bended_state_dict = bended.bended_state_dict()\n",
    "\n",
    "print(\"-- original state dict :\\n\", original_state_dict['conv_modules.0.weight'])\n",
    "print(\"-- bended state dict: \\n\", bended_state_dict['conv_modules.0.weight'])\n",
    "\n",
    "original_graph = bended.graph()\n",
    "bended_graph = bended.bend_graph()\n",
    "\n",
    "print(\"-- original graph :\\n\", original_graph)\n",
    "print(\"-- bended graph: \\n\", bended_graph)\n",
    "\n",
    "x = torch.randn(4, 1, 128)\n",
    "original_activation = bended.get_activations(\"nnlin\", x=x, bended=False)\n",
    "bended_activation = bended.get_activations(\"nnlin\", x=x)\n",
    "print(original_activation['nnlin'].shape)\n",
    "print(bended_activation['nnlin'].shape)\n",
    "print(\"-- original activation :\\n\", original_activation['nnlin'][0, 0])\n",
    "print(\"-- bended activation: \\n\", bended_activation['nnlin'][0, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common issues with activation bending and shapes\n",
    "\n",
    "Let's see now a critical case of activation bending : shape handling. Indeed, let's try to change the shape of the input, and apply our bending operations : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of tensor a (60) must match the size of tensor b (124) at non-singleton dimension 2\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(1, 1, 64)\n",
    "try: \n",
    "    out = bended.forward(x)\n",
    "except RuntimeError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This happens because, during tracing, activation `nnlin` has been recorded to have shape `torch.Size([4, 8, 124])`. Hence, the `Mask` callback has been initialiazed with a similar shape, causing a `RuntimeError` during multiplication. To make this bending shape independant on last dimension, we can only mask the channel dimension : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "bended.reset()\n",
    "bended.bend(tb.Mask(prob=0.3, dim=-2), \"nnlin\")\n",
    "\n",
    "x = torch.randn(1, 1, 64)\n",
    "out = bended.forward(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Activation bending is indeed allowed by our improved tracer, that records shapes of activations during graph tracing. This extension asks, in exchange, to be precautionary on how you bend the graph to be sure that no improper bending operations are applied during the execution process. For more information of this, jump to the next tutorial!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monitoring bending operations with `BendingConfig`\n",
    "\n",
    "Bending operations of a `BendingModule` can be objectified using the `BendingConfig` object, that can also be used for bending, pickling, and monitoring bending operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BendingConfig(\n",
      "module = Stanley(id = 4699713232)\n",
      "\tMask(prob=0.400): ['conv_modules.0.weight', 'nnlin']\n",
      "\tMask(prob=0.600): ['conv_modules.0.weight', 'conv_modules.1.weight']\n",
      ")\n",
      "bending config :  BendingConfig(\n",
      "module = Stanley(id = 4699713232)\n",
      "\tMask(prob=0.400): ['conv_modules.0.weight', 'nnlin']\n",
      "\tMask(prob=0.600): ['conv_modules.0.weight', 'conv_modules.1.weight']\n",
      ")\n",
      "operations for key conv_modules.0.weight: [Mask(prob=0.400), Mask(prob=0.600)]\n",
      "loaded config :  BendingConfig(\n",
      "module = Stanley(id = 4699713232)\n",
      "\tMask(prob=0.400): ['conv_modules.0.weight', 'nnlin']\n",
      "\tMask(prob=0.600): ['conv_modules.0.weight', 'conv_modules.1.weight']\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/domkirke/miniconda3/envs/ml2/lib/python3.11/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(io.BytesIO(b))\n"
     ]
    }
   ],
   "source": [
    "module = Stanley()\n",
    "bended = tb.BendedModule(module)\n",
    "bended.trace(x=torch.randn(1, 1, 1024))\n",
    "\n",
    "bended.bend(tb.Mask(prob=0.4), \"conv_modules.0.weight\", \"nnlin\")\n",
    "bended.bend(tb.Mask(prob=0.6), \"conv_modules.0.weight\", \"conv_modules.1.weight\")\n",
    "\n",
    "bending_config = bended.bending_config();\n",
    "print(\"bending config : \", bending_config)\n",
    "key = \"conv_modules.0.weight\"\n",
    "print(f\"operations for key {key}:\", bending_config.op_from_key(key))\n",
    "\n",
    "bending_config.save('test.tb')\n",
    "bending_config = tb.BendingConfig.load('test.tb', module=bended)\n",
    "print(\"loaded config : \", bending_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`BendingConfig` may be created out of the blue, or bounded to a given `BendedModule` for automatic key resolution. They can also be added or compared together :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before binding : \n",
      "config1 : BendingConfig(\n",
      "\t(Mask(prob=0.800), 'conv_modules.\\\\d.weight')\n",
      "\t(Mask(prob=0.800), 'nnlin')\n",
      ")\n",
      "config2 : BendingConfig(\n",
      "\t(Mask(prob=0.800), 'conv_modules.\\\\d.weight')\n",
      "\t(Mask(prob=0.800), 'nnlin')\n",
      ")\n",
      "True\n",
      "\n",
      "After binding : \n",
      "config1 : BendingConfig(\n",
      "module = Stanley(id = 4699713232)\n",
      "\tMask(prob=0.800): ['conv_modules.0.weight', 'conv_modules.1.weight', 'nnlin']\n",
      ")\n",
      "config2 : BendingConfig(\n",
      "module = Stanley(id = 4699713232)\n",
      "\tMask(prob=0.800): ['conv_modules.0.weight', 'conv_modules.1.weight', 'nnlin']\n",
      ")\n",
      "BendingConfig(\n",
      "module = Stanley(id = 4699713232)\n",
      "\tMask(prob=0.800): ['conv_modules.0.weight', 'conv_modules.1.weight', 'nnlin']\n",
      ")\n",
      "\n",
      "Comparison : \n",
      "True\n"
     ]
    }
   ],
   "source": [
    "cb1 = tb.Mask(0.8)\n",
    "config1 = tb.BendingConfig(\n",
    "    (cb1, \"conv_modules.\\d.weight\"),\n",
    "    (cb1, \"nnlin\")\n",
    ")\n",
    "config2 = tb.BendingConfig((cb1, \"conv_modules.\\d.weight\")) + tb.BendingConfig((cb1, \"nnlin\"))\n",
    "\n",
    "print('Before binding : ')\n",
    "print(\"config1 :\", config1)\n",
    "print(\"config2 :\", config2)\n",
    "print(config1 == config2)\n",
    "\n",
    "config1.bind(bended)\n",
    "config2.bind(bended)\n",
    "\n",
    "print('\\nAfter binding : ')\n",
    "print(\"config1 :\", config1)\n",
    "print(\"config2 :\", config2)\n",
    "\n",
    "bended.reset()\n",
    "bended.bend(config1)\n",
    "module_config = bended.bending_config()\n",
    "print('\\nComparison : ')\n",
    "print(config1 == module_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Versions and interpolation\n",
    "\n",
    "`BendedModule` also allows you to create several versions of the original module, and to interpolate between them in a smooth manner. Though, beware that this only works with weight bending as smooth interpolation between graphs does not really make sense! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "default version :  _default\n",
      "current version :  bended\n",
      "current version :  _default\n",
      "original == bended :  tensor(False)\n",
      "original == interpolated :  tensor(False)\n",
      "bended == interpolated :  tensor(False)\n"
     ]
    }
   ],
   "source": [
    "module = Stanley()\n",
    "bended = tb.BendedModule(module)\n",
    "\n",
    "x = torch.randn(1, 1, 128)\n",
    "out_unbended = bended(x)\n",
    "\n",
    "print(\"default version : \", bended.version)\n",
    "bended.bend(tb.Mask(prob=0.3), \"conv_modules.0.weight\")\n",
    "bended.write(\"bended\")\n",
    "print(\"current version : \", bended.version)\n",
    "\n",
    "# revert to default\n",
    "bended.version = None\n",
    "print('current version : ', bended.version)\n",
    "\n",
    "with bended.set_version():\n",
    "    out_original = bended(x)\n",
    "with bended.set_version(\"bended\"):\n",
    "    out_bended = bended(x)\n",
    "\n",
    "# arguments of bended.interpolate has an optional positional argument\n",
    "# for default configuration weight, plus keyword arguments for every additional\n",
    "# config weights. \n",
    "with bended.interpolate(1., bended=1.):\n",
    "    out_interpolated = bended(x)\n",
    "\n",
    "print(\"original == bended : \", (out_original == out_bended).all())\n",
    "print(\"original == interpolated : \", (out_original == out_interpolated).all())\n",
    "print(\"bended == interpolated : \", (out_bended == out_interpolated).all())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also load another version of the same module, provided that their state dict is the same, and interpolate between them with the `interpolate` context manager."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original == imported :  tensor(False)\n",
      "original == interpolated :  tensor(False)\n",
      "imported == interpolated :  tensor(False)\n",
      "given state dict has different keys ; you can bypass this warning by setting strict=False, at your own risk\n"
     ]
    }
   ],
   "source": [
    "module = Stanley()\n",
    "module2 = Stanley()\n",
    "bended = tb.BendedModule(module)\n",
    "bended.create_version(\"imported\", module2)\n",
    "\n",
    "x = torch.randn(1, 1, 128)\n",
    "with bended.set_version():\n",
    "    out_original = bended(x)\n",
    "with bended.set_version(\"imported\"):\n",
    "    out_imported = bended(x)\n",
    "with bended.interpolate(1., imported=1.):\n",
    "    out_interpolated = bended(x)\n",
    "\n",
    "print(\"original == imported : \", (out_original == out_imported).all())\n",
    "print(\"original == interpolated : \", (out_original == out_interpolated).all())\n",
    "print(\"imported == interpolated : \", (out_imported == out_interpolated).all())\n",
    "\n",
    "class Doppleganger(nn.Module):\n",
    "    pass\n",
    "\n",
    "try: \n",
    "    bended.create_version(\"imported\", Doppleganger())\n",
    "except tb.BendingError as e: \n",
    "    print(e)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
