{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extended tracing\n",
    "\n",
    "We will now dive into `torchbend`'s extended tracer. Let's take this weird `nn.Module`, which is totally useless besides demonstrating how `torchbend`'s tracer extends the original `torch.fx`'s one and experiment tracing and bending."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.fx error :  symbolically traced variables cannot be used as inputs to control flow\n",
      "torchbend graph: \n",
      "graph():\n",
      "    %x : torch.Tensor [num_users=3] = placeholder[target=x]\n",
      "    %getattr_1 : [num_users=1] = call_function[target=builtins.getattr](args = (%x, shape), kwargs = {})\n",
      "    %getitem : int [num_users=1] = call_function[target=operator.getitem](args = (%getattr_1, 1), kwargs = {})\n",
      "    %gt : [num_users=0] = call_function[target=operator.gt](args = (%getitem, 1), kwargs = {})\n",
      "    %getattr_2 : [num_users=1] = call_function[target=builtins.getattr](args = (%x, shape), kwargs = {})\n",
      "    %getitem_1 : int [num_users=0] = call_function[target=operator.getitem](args = (%getattr_2, 0), kwargs = {})\n",
      "    %getitem_2 : [num_users=1] = call_function[target=operator.getitem](args = (%x, [0]), kwargs = {})\n",
      "    %conv_module : [num_users=1] = call_module[target=conv_module](args = (%getitem_2,), kwargs = {})\n",
      "    %batch_norm : [num_users=1] = call_module[target=batch_norm](args = (%conv_module,), kwargs = {})\n",
      "    %add : [num_users=1] = call_function[target=operator.add](args = (0, %batch_norm), kwargs = {})\n",
      "    return add\n"
     ]
    }
   ],
   "source": [
    "import torch, torch.nn as nn\n",
    "import sys; sys.path.append(\"..\")\n",
    "import torchbend as tb\n",
    "\n",
    "class Doug(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv_module = nn.Conv1d(1, 16, 3)\n",
    "        self.batch_norm = nn.BatchNorm1d(16)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor):\n",
    "        if x.shape[1] > 1: \n",
    "            return torch.cat([self.forward(x[:, [i]]) for i in range(x.shape[1])], -1)\n",
    "        else:\n",
    "            outs = []\n",
    "            for i in range(x.shape[0]):\n",
    "                out_tmp = self.conv_module(x[[i]])\n",
    "                out_tmp = self.batch_norm(out_tmp)\n",
    "                outs.append(out_tmp)\n",
    "            return sum(outs)\n",
    "\n",
    "module = Doug()\n",
    "\n",
    "try: \n",
    "    torch.fx.symbolic_trace(module)\n",
    "except torch.fx.proxy.TraceError as e:\n",
    "    print('torch.fx error : ', e)\n",
    "\n",
    "bended = tb.BendedModule(module)\n",
    "bended.trace(x=torch.randn(1, 1, 16))\n",
    "print('torchbend graph: ')\n",
    "print(bended.graph())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happens here? `torch.fx` is a pure symbolic tracer, that feeds `Proxy` items as the input to record every operation of a computing graph without actually processing it. While this has many advantages, it also prevents several operations that depends on the concrete value of the arguments, as here iterating through the shape of the input argument. `torchbend` alleviates this by doubling this pure symbolical tracing with a parallel execution process, at the cost of giving the input argument `x`. \n",
    "\n",
    "![tracing process](img/tracing.png \"Tracing process\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hardcoded control-flow\n",
    "\n",
    "While this doubled tracing process to does not limit the original `torch.fx` tracer, it may ask some cautions (similarly to how [jax](https://jax.readthedocs.io/en/latest/notebooks/Common_Gotchas_in_JAX.html) handles computational graphs). Indeed, the control flow is some how \"hardcoded\" in the graph, such that different inputs may lead to different graphs :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graph():\n",
      "    %x : torch.Tensor [num_users=3] = placeholder[target=x]\n",
      "    %getattr_1 : [num_users=1] = call_function[target=builtins.getattr](args = (%x, shape), kwargs = {})\n",
      "    %getitem : int [num_users=1] = call_function[target=operator.getitem](args = (%getattr_1, 1), kwargs = {})\n",
      "    %gt : [num_users=0] = call_function[target=operator.gt](args = (%getitem, 1), kwargs = {})\n",
      "    %getattr_2 : [num_users=1] = call_function[target=builtins.getattr](args = (%x, shape), kwargs = {})\n",
      "    %getitem_1 : int [num_users=0] = call_function[target=operator.getitem](args = (%getattr_2, 0), kwargs = {})\n",
      "    %getitem_2 : [num_users=1] = call_function[target=operator.getitem](args = (%x, [0]), kwargs = {})\n",
      "    %conv_module : [num_users=1] = call_module[target=conv_module](args = (%getitem_2,), kwargs = {})\n",
      "    %batch_norm : [num_users=1] = call_module[target=batch_norm](args = (%conv_module,), kwargs = {})\n",
      "    %add : [num_users=1] = call_function[target=operator.add](args = (0, %batch_norm), kwargs = {})\n",
      "    return add\n",
      "graph():\n",
      "    %x : torch.Tensor [num_users=4] = placeholder[target=x]\n",
      "    %getattr_1 : [num_users=1] = call_function[target=builtins.getattr](args = (%x, shape), kwargs = {})\n",
      "    %getitem : int [num_users=1] = call_function[target=operator.getitem](args = (%getattr_1, 1), kwargs = {})\n",
      "    %gt : [num_users=0] = call_function[target=operator.gt](args = (%getitem, 1), kwargs = {})\n",
      "    %getattr_2 : [num_users=1] = call_function[target=builtins.getattr](args = (%x, shape), kwargs = {})\n",
      "    %getitem_1 : int [num_users=0] = call_function[target=operator.getitem](args = (%getattr_2, 1), kwargs = {})\n",
      "    %getitem_2 : [num_users=3] = call_function[target=operator.getitem](args = (%x, (slice(None, None, None), [0])), kwargs = {})\n",
      "    %getattr_3 : [num_users=1] = call_function[target=builtins.getattr](args = (%getitem_2, shape), kwargs = {})\n",
      "    %getitem_3 : int [num_users=1] = call_function[target=operator.getitem](args = (%getattr_3, 1), kwargs = {})\n",
      "    %gt_1 : [num_users=0] = call_function[target=operator.gt](args = (%getitem_3, 1), kwargs = {})\n",
      "    %getattr_4 : [num_users=1] = call_function[target=builtins.getattr](args = (%getitem_2, shape), kwargs = {})\n",
      "    %getitem_4 : int [num_users=0] = call_function[target=operator.getitem](args = (%getattr_4, 0), kwargs = {})\n",
      "    %getitem_5 : [num_users=1] = call_function[target=operator.getitem](args = (%getitem_2, [0]), kwargs = {})\n",
      "    %conv_module : [num_users=1] = call_module[target=conv_module](args = (%getitem_5,), kwargs = {})\n",
      "    %batch_norm : [num_users=1] = call_module[target=batch_norm](args = (%conv_module,), kwargs = {})\n",
      "    %add : [num_users=1] = call_function[target=operator.add](args = (0, %batch_norm), kwargs = {})\n",
      "    %getitem_6 : [num_users=3] = call_function[target=operator.getitem](args = (%x, (slice(None, None, None), [1])), kwargs = {})\n",
      "    %getattr_5 : [num_users=1] = call_function[target=builtins.getattr](args = (%getitem_6, shape), kwargs = {})\n",
      "    %getitem_7 : int [num_users=1] = call_function[target=operator.getitem](args = (%getattr_5, 1), kwargs = {})\n",
      "    %gt_2 : [num_users=0] = call_function[target=operator.gt](args = (%getitem_7, 1), kwargs = {})\n",
      "    %getattr_6 : [num_users=1] = call_function[target=builtins.getattr](args = (%getitem_6, shape), kwargs = {})\n",
      "    %getitem_8 : int [num_users=0] = call_function[target=operator.getitem](args = (%getattr_6, 0), kwargs = {})\n",
      "    %getitem_9 : [num_users=1] = call_function[target=operator.getitem](args = (%getitem_6, [0]), kwargs = {})\n",
      "    %conv_module_1 : [num_users=1] = call_module[target=conv_module](args = (%getitem_9,), kwargs = {})\n",
      "    %batch_norm_1 : [num_users=1] = call_module[target=batch_norm](args = (%conv_module_1,), kwargs = {})\n",
      "    %add_1 : [num_users=1] = call_function[target=operator.add](args = (0, %batch_norm_1), kwargs = {})\n",
      "    %cat : [num_users=1] = call_function[target=torch.cat](args = ([%add, %add_1], -1), kwargs = {})\n",
      "    return cat\n"
     ]
    }
   ],
   "source": [
    "module = Doug()\n",
    "\n",
    "bended = tb.BendedModule(module)\n",
    "bended.trace(x=torch.randn(1, 1, 16))\n",
    "print(bended.graph())\n",
    "\n",
    "bended.trace(x=torch.randn(1, 2, 16))\n",
    "print(bended.graph())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that, in the second case, the loop is \"hardcoded\" in the graph. The graph is then different, and the result may change with the same input : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raised error :  Given groups=1, weight of size [16, 1, 3], expected input[1, 2, 16] to have 1 channels, but got 2 channels instead\n",
      "raised error : index 1 is out of bounds for dimension 0 with size 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/domkirke/miniconda3/envs/ml2/lib/python3.11/site-packages/torch/fx/graph_module.py\", line 303, in __call__\n",
      "    return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/domkirke/miniconda3/envs/ml2/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/domkirke/miniconda3/envs/ml2/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"<eval_with_key>.12\", line 20, in forward\n",
      "    getitem_6 = x[(slice(None, None, None), [1])];  x = None\n",
      "                ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "IndexError: index 1 is out of bounds for dimension 0 with size 1\n",
      "\n",
      "Call using an FX-traced Module, line 20 of the traced Module's generated forward function:\n",
      "    add = 0 + batch_norm;  batch_norm = None\n",
      "    getitem_6 = x[(slice(None, None, None), [1])];  x = None\n",
      "\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE\n",
      "    getattr_5 = getitem_6.shape\n",
      "\n",
      "    getitem_7 : int = getattr_5[1];  getattr_5 = None\n",
      "\n"
     ]
    }
   ],
   "source": [
    "module = Doug()\n",
    "\n",
    "bended = tb.BendedModule(module)\n",
    "bended.trace(x=torch.randn(1, 1, 16))\n",
    "bended(torch.randn(1, 1, 16))\n",
    "try: \n",
    "    bended(torch.randn(1, 2, 16))\n",
    "except Exception as e:\n",
    "    print(\"raised error : \", e)\n",
    "    \n",
    "\n",
    "bended.trace(x=torch.randn(1, 2, 16))\n",
    "bended(torch.randn(1, 2, 16))\n",
    "try: \n",
    "    bended(torch.randn(1, 1, 16))\n",
    "except Exception as e:\n",
    "    print(\"raised error :\", e)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each steps is recorded into the graph, and can be retrieved by accessing the `flow_steps` attribute of the corresponding `torch.fx.Graph` object : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[LogicalFlowStep(name=gt, value=True, file=/var/folders/vk/nn1706pd25b57gxz9y3p1wqh0000gn/T/ipykernel_87887/1390080083.py:forward.11)),\n",
       " LogicalFlowStep(name=gt_1, value=False, file=/var/folders/vk/nn1706pd25b57gxz9y3p1wqh0000gn/T/ipykernel_87887/1390080083.py:forward.11)),\n",
       " LogicalFlowStep(name=gt_2, value=False, file=/var/folders/vk/nn1706pd25b57gxz9y3p1wqh0000gn/T/ipykernel_87887/1390080083.py:forward.11))]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bended.graph().flow_steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tracer extensions : shape, loop, logical flow\n",
    "\n",
    "So far three operations that are not allowed by original `torch.fx.Tracer` are implemented : shape attribues, logical control flow, and loops. Let's see that we these specific modules : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.fx error : symbolically traced variables cannot be used as inputs to control flow\n",
      "(None,) [LogicalFlowStep(name=all_1, value=False, file=/var/folders/vk/nn1706pd25b57gxz9y3p1wqh0000gn/T/ipykernel_87887/2232374878.py:forward.8))]\n",
      "(tensor([4., 4., 4., 4.]),) []\n",
      "critical case :  tensor([6., 6., 6., 6., 6., 6.])\n"
     ]
    }
   ],
   "source": [
    "class LoopFoo(nn.Module):\n",
    "    def forward(self, x, n: int):\n",
    "        for i in range(n):\n",
    "            x = x * x\n",
    "        return x\n",
    "\n",
    "class LogicalFoo(nn.Module):\n",
    "    def forward(self, x):\n",
    "        if x.all():\n",
    "            return 1\n",
    "        else: \n",
    "            return 0\n",
    "\n",
    "class ShapeFoo(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x * x.shape[0]\n",
    "\n",
    "#TODO does not work\n",
    "# foo = LoopFoo()\n",
    "# try: \n",
    "#     mod = torch.fx.symbolic_trace(foo)\n",
    "# except torch.fx.proxy.TraceError as e:\n",
    "#     print(e)\n",
    "# bended = tb.BendedModule(foo)\n",
    "# graph, out = bended.trace(x=torch.tensor(2), n=4, _return_out=True)\n",
    "# print(out, graph.flow_steps)\n",
    "\n",
    "foo = LogicalFoo()\n",
    "try: \n",
    "    mod = torch.fx.symbolic_trace(foo)\n",
    "except torch.fx.proxy.TraceError as e:\n",
    "    print(\"torch.fx error :\", e)\n",
    "bended = tb.BendedModule(foo)\n",
    "graph, out = bended.trace(x=torch.tensor(0), _return_out=True)\n",
    "print(out, graph.flow_steps)\n",
    "\n",
    "\n",
    "foo = ShapeFoo()\n",
    "try: \n",
    "    mod = torch.fx.symbolic_trace(foo)\n",
    "except torch.fx.proxy.TraceError as e:\n",
    "    print(\"torch.fx error :\", e)\n",
    "bended = tb.BendedModule(foo)\n",
    "graph, out = bended.trace(x=torch.ones(4), _return_out=True)\n",
    "print(out, graph.flow_steps)\n",
    "# example critical case\n",
    "out_ok = bended(torch.ones(6))\n",
    "print('ok case : ', out_ok)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cautions with activation bending\n",
    "\n",
    "This hard-coded graph for critical values can also impact some activation bending callbacks, that have to initialize their internal states with a given shape. For example, let's take two different bending callbacks : \n",
    "- `tb.Mask`, that initializes a binary mask for a given target (hence requiring a shape), \n",
    "- `tb.bias`, that biases a given target with a static scalar value (hence not requiring a shape)\n",
    "\n",
    "We can see that the former cannot be adapted to a change of shape, while the second can. Such issues may be alleviated by carefully adapting the bending callback, as we do here by maksing only specific channels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------  -----------  ----------------------\n",
      "x            placeholder  torch.Size([1, 1, 16])\n",
      "conv_module  call_module  torch.Size([1, 4, 14])\n",
      "-----------  -----------  ----------------------\n",
      "\n",
      "-- Original out\n",
      "tensor([[[-1.0954, -0.4893,  0.1828, -0.0825, -0.8288, -0.8804,  0.5599,\n",
      "          -0.1416, -1.2253, -0.0266, -0.0884, -0.4542,  0.2300, -1.3383],\n",
      "         [ 0.4717, -0.3082, -0.4564, -0.3030,  0.6222, -0.1784, -0.9403,\n",
      "           0.2578,  0.3855, -0.7710,  0.3001, -0.7324,  0.0178,  0.6979],\n",
      "         [-0.1401, -0.3743, -0.2121,  0.2424,  0.1981, -0.0893, -0.4107,\n",
      "           0.3499,  0.3239, -0.4214, -0.0099,  0.2867, -0.1838,  0.4334],\n",
      "         [-0.8375, -0.4086, -0.5440, -0.0560, -0.9016,  0.0270, -0.3381,\n",
      "          -0.7345, -0.0863, -0.1345, -1.2418,  0.9393, -1.2654, -0.3447]]],\n",
      "       grad_fn=<ConvolutionBackward0>)\n",
      "\n",
      "-- Bias\n",
      "tensor([[[-0.0954,  0.5107,  1.1828,  0.9175,  0.1712,  0.1196,  1.5599,\n",
      "           0.8584, -0.2253,  0.9734,  0.9116,  0.5458,  1.2300, -0.3383],\n",
      "         [ 1.4717,  0.6918,  0.5436,  0.6970,  1.6222,  0.8216,  0.0597,\n",
      "           1.2578,  1.3855,  0.2290,  1.3001,  0.2676,  1.0178,  1.6979],\n",
      "         [ 0.8599,  0.6257,  0.7879,  1.2424,  1.1981,  0.9107,  0.5893,\n",
      "           1.3499,  1.3239,  0.5786,  0.9901,  1.2867,  0.8162,  1.4334],\n",
      "         [ 0.1625,  0.5914,  0.4560,  0.9440,  0.0984,  1.0270,  0.6619,\n",
      "           0.2655,  0.9137,  0.8655, -0.2418,  1.9393, -0.2654,  0.6553]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "\n",
      "-- Mask\n",
      "tensor([[[-1.0954, -0.0000,  0.0000, -0.0000, -0.0000, -0.0000,  0.5599,\n",
      "          -0.0000, -0.0000, -0.0000, -0.0884, -0.0000,  0.2300, -1.3383],\n",
      "         [ 0.0000, -0.0000, -0.0000, -0.0000,  0.0000, -0.0000, -0.0000,\n",
      "           0.0000,  0.3855, -0.7710,  0.0000, -0.0000,  0.0178,  0.0000],\n",
      "         [-0.0000, -0.0000, -0.2121,  0.0000,  0.0000, -0.0893, -0.4107,\n",
      "           0.3499,  0.0000, -0.0000, -0.0099,  0.0000, -0.1838,  0.0000],\n",
      "         [-0.0000, -0.0000, -0.0000, -0.0000, -0.9016,  0.0000, -0.3381,\n",
      "          -0.0000, -0.0863, -0.0000, -1.2418,  0.0000, -1.2654, -0.0000]]],\n",
      "       grad_fn=<MulBackward0>)\n",
      "Error with different shape :  The size of tensor a (30) must match the size of tensor b (14) at non-singleton dimension 2\n",
      "\n",
      "-- Channeled mask\n",
      "tensor([[[-1.0954, -0.4893,  0.1828, -0.0825, -0.8288, -0.8804,  0.5599,\n",
      "          -0.1416, -1.2253, -0.0266, -0.0884, -0.4542,  0.2300, -1.3383],\n",
      "         [ 0.0000, -0.0000, -0.0000, -0.0000,  0.0000, -0.0000, -0.0000,\n",
      "           0.0000,  0.0000, -0.0000,  0.0000, -0.0000,  0.0000,  0.0000],\n",
      "         [-0.0000, -0.0000, -0.0000,  0.0000,  0.0000, -0.0000, -0.0000,\n",
      "           0.0000,  0.0000, -0.0000, -0.0000,  0.0000, -0.0000,  0.0000],\n",
      "         [-0.0000, -0.0000, -0.0000, -0.0000, -0.0000,  0.0000, -0.0000,\n",
      "          -0.0000, -0.0000, -0.0000, -0.0000,  0.0000, -0.0000, -0.0000]]],\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "class Greg(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv_module = nn.Conv1d(1, 4, 3)\n",
    "    def forward(self, x):\n",
    "        return self.conv_module(x)\n",
    "\n",
    "module = Greg()\n",
    "bended = tb.BendedModule(module)\n",
    "x = torch.randn(1, 1, 16)\n",
    "graph, out = bended.trace(x=x, _return_out=True)\n",
    "bended.print_activations()\n",
    "print('\\n-- Original out')\n",
    "print(out[0])\n",
    "\n",
    "# bending with Scale\n",
    "#TODO wtf\n",
    "print('\\n-- Bias')\n",
    "bended.bend(tb.Bias(1.), \"conv_module$\")\n",
    "print(bended(x))\n",
    "bended(torch.randn(4, 1, 32)); # -> OK\n",
    "\n",
    "print('\\n-- Mask')\n",
    "bended.reset()\n",
    "bended.bend(tb.Mask(0.3), \"conv_module$\")\n",
    "print(bended(x))\n",
    "try:\n",
    "    bended(torch.randn(4, 1, 32)); # -> not OK\n",
    "except Exception as e:\n",
    "    print(\"Error with different shape : \", e)\n",
    "\n",
    "\n",
    "print('\\n-- Channeled mask')\n",
    "bended.reset()\n",
    "bended.bend(tb.Mask(prob=0.3, dim=-2), \"conv_module$\")\n",
    "print(bended(x))\n",
    "bended(torch.randn(4, 1, 32)); # -> OK\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
