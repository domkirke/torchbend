{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining callbacks\n",
    "\n",
    "\n",
    "We will see in this tutorial how to define a bending callaback to perform custom callback operations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining stateless callbacks\n",
    "\n",
    "To start easy, let's define a callback that does not need any internal buffer. A bending callback must define two functions : \n",
    "- the `forward` function, that is used for activation bending\n",
    "- the `apply_to_param` function, that is used for weight bending\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.0477, -0.3881,  0.8609, -0.2017,  0.3453, -0.2270,  0.1903,\n",
      "           0.3508, -0.3234,  0.2075,  0.2420, -0.5515,  0.1830, -0.2658,\n",
      "           0.1675,  0.1738,  0.1183,  0.0775, -0.0210,  0.0071,  0.1731,\n",
      "          -0.1167, -0.3698,  0.5692,  0.1424, -0.1753,  0.0266,  0.2850]]],\n",
      "       grad_fn=<ConvolutionBackward0>)\n",
      "tensor([[[-0.0461,  0.0040, -0.0027,  0.0541,  0.0488,  0.0567,  0.0513,\n",
      "           0.0700,  0.0420,  0.0533,  0.0543,  0.0456, -0.0016, -0.0268,\n",
      "          -0.0409, -0.0141,  0.0076,  0.0398,  0.0332, -0.0019,  0.0298,\n",
      "          -0.0193, -0.0367, -0.1067,  0.0098, -0.0542,  0.0266, -0.0605]]],\n",
      "       grad_fn=<ConvolutionBackward0>)\n",
      "caca\n",
      "caca\n",
      "caca\n",
      "caca\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "\nArguments for call are not valid.\nThe following variants are available:\n  \n  aten::mul.Tensor(Tensor self, Tensor other) -> Tensor:\n  Expected a value of type 'Tensor' for argument 'self' but instead found type 'Optional[Tensor]'.\n  \n  aten::mul.Scalar(Tensor self, Scalar other) -> Tensor:\n  Expected a value of type 'Tensor' for argument 'self' but instead found type 'Optional[Tensor]'.\n  \n  aten::mul.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!):\n  Expected a value of type 'Tensor' for argument 'self' but instead found type 'Optional[Tensor]'.\n  \n  aten::mul.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!):\n  Expected a value of type 'Tensor' for argument 'self' but instead found type 'Optional[Tensor]'.\n  \n  aten::mul.left_t(t[] l, int n) -> t[]:\n  Could not match type Optional[Tensor] to List[t] in argument 'l': Cannot match List[t] to Optional[Tensor].\n  \n  aten::mul.right_(int n, t[] l) -> t[]:\n  Expected a value of type 'int' for argument 'n' but instead found type 'Optional[Tensor]'.\n  \n  aten::mul.int(int a, int b) -> int:\n  Expected a value of type 'int' for argument 'a' but instead found type 'Optional[Tensor]'.\n  \n  aten::mul.complex(complex a, complex b) -> complex:\n  Expected a value of type 'complex' for argument 'a' but instead found type 'Optional[Tensor]'.\n  \n  aten::mul.float(float a, float b) -> float:\n  Expected a value of type 'float' for argument 'a' but instead found type 'Optional[Tensor]'.\n  \n  aten::mul.int_complex(int a, complex b) -> complex:\n  Expected a value of type 'int' for argument 'a' but instead found type 'Optional[Tensor]'.\n  \n  aten::mul.complex_int(complex a, int b) -> complex:\n  Expected a value of type 'complex' for argument 'a' but instead found type 'Optional[Tensor]'.\n  \n  aten::mul.float_complex(float a, complex b) -> complex:\n  Expected a value of type 'float' for argument 'a' but instead found type 'Optional[Tensor]'.\n  \n  aten::mul.complex_float(complex a, float b) -> complex:\n  Expected a value of type 'complex' for argument 'a' but instead found type 'Optional[Tensor]'.\n  \n  aten::mul.int_float(int a, float b) -> float:\n  Expected a value of type 'int' for argument 'a' but instead found type 'Optional[Tensor]'.\n  \n  aten::mul.float_int(float a, int b) -> float:\n  Expected a value of type 'float' for argument 'a' but instead found type 'Optional[Tensor]'.\n  \n  aten::mul(Scalar a, Scalar b) -> Scalar:\n  Expected a value of type 'number' for argument 'a' but instead found type 'Optional[Tensor]'.\n  \n  mul(float a, Tensor b) -> Tensor:\n  Expected a value of type 'float' for argument 'a' but instead found type 'Optional[Tensor]'.\n  \n  mul(int a, Tensor b) -> Tensor:\n  Expected a value of type 'int' for argument 'a' but instead found type 'Optional[Tensor]'.\n  \n  mul(complex a, Tensor b) -> Tensor:\n  Expected a value of type 'complex' for argument 'a' but instead found type 'Optional[Tensor]'.\n\nThe original call is:\n  File \"/var/folders/vk/nn1706pd25b57gxz9y3p1wqh0000gn/T/ipykernel_45941/43206288.py\", line 31\n        - cache : (optional): callbacks cache the original parameter value for dynamical modification\n        \"\"\"\n        param.set_(cache * cache)\n                   ~~~~~~~~~~~~~ <--- HERE\n'Square.apply_to_param' is being compiled since it was called from 'Square.apply'\n  File \"/Users/domkirke/Dropbox/code/torchbend/docs/../torchbend/bending/base.py\", line 168\n        for i, v in enumerate(self._bending_targets):\n            v_cached = self.cache_from_id(i).data\n            self.apply_to_param(i, v, v_cached)\n            ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE\n'Square.apply' is being compiled since it was called from 'ScriptedBendedModule._update_weights'\n  File \"/Users/domkirke/Dropbox/code/torchbend/docs/../torchbend/tracing/script.py\", line 150\n            for i, c in enumerate(self._bending_callbacks):\n                for j in callbacks:\n                    if i == j: c.apply()\n                               ~~~~~~~ <--- HERE\n'ScriptedBendedModule._update_weights' is being compiled since it was called from 'ScriptedBendedModule._set_bending_control'\n  File \"/Users/domkirke/Dropbox/code/torchbend/docs/../torchbend/tracing/script.py\", line 167\n            if v.name == name:\n                v.set_value(value)\n        self._update_weights(name)\n        ~~~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE\n        return 0\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 52\u001b[0m\n\u001b[1;32m     49\u001b[0m out \u001b[38;5;241m=\u001b[39m bended(x)\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28mprint\u001b[39m(out)\n\u001b[0;32m---> 52\u001b[0m scripted \u001b[38;5;241m=\u001b[39m \u001b[43mbended\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscript\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Dropbox/code/torchbend/docs/../torchbend/tracing/__init__.py:14\u001b[0m, in \u001b[0;36mscript_method\u001b[0;34m(self, script, export_for_nn, **kwargs)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# for m in checklist(methods):\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m#     setattr(mod, m, torch.jit.export(getattr(mod, m)))\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m script: \n\u001b[0;32m---> 14\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscript\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmod\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m: \n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m mod\n",
      "File \u001b[0;32m~/miniconda3/envs/ml2/lib/python3.11/site-packages/torch/jit/_script.py:1432\u001b[0m, in \u001b[0;36mscript\u001b[0;34m(obj, optimize, _frames_up, _rcb, example_inputs)\u001b[0m\n\u001b[1;32m   1429\u001b[0m _TOPLEVEL \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   1431\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1432\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_script_impl\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1433\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1434\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptimize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1435\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_frames_up\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_frames_up\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1436\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_rcb\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_rcb\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1437\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexample_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexample_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1438\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1439\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1440\u001b[0m     _TOPLEVEL \u001b[38;5;241m=\u001b[39m prev\n",
      "File \u001b[0;32m~/miniconda3/envs/ml2/lib/python3.11/site-packages/torch/jit/_script.py:1146\u001b[0m, in \u001b[0;36m_script_impl\u001b[0;34m(obj, optimize, _frames_up, _rcb, example_inputs)\u001b[0m\n\u001b[1;32m   1144\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mModule):\n\u001b[1;32m   1145\u001b[0m     obj \u001b[38;5;241m=\u001b[39m call_prepare_scriptable_func(obj)\n\u001b[0;32m-> 1146\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_recursive\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_script_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1147\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_recursive\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfer_methods_to_compile\u001b[49m\n\u001b[1;32m   1148\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1149\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1150\u001b[0m     obj \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39m__prepare_scriptable__() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(obj, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__prepare_scriptable__\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m obj  \u001b[38;5;66;03m# type: ignore[operator]\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/ml2/lib/python3.11/site-packages/torch/jit/_recursive.py:563\u001b[0m, in \u001b[0;36mcreate_script_module\u001b[0;34m(nn_module, stubs_fn, share_types, is_tracing)\u001b[0m\n\u001b[1;32m    561\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_tracing:\n\u001b[1;32m    562\u001b[0m     AttributeTypeIsSupportedChecker()\u001b[38;5;241m.\u001b[39mcheck(nn_module)\n\u001b[0;32m--> 563\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcreate_script_module_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnn_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconcrete_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstubs_fn\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ml2/lib/python3.11/site-packages/torch/jit/_recursive.py:640\u001b[0m, in \u001b[0;36mcreate_script_module_impl\u001b[0;34m(nn_module, concrete_type, stubs_fn)\u001b[0m\n\u001b[1;32m    638\u001b[0m \u001b[38;5;66;03m# Compile methods if necessary\u001b[39;00m\n\u001b[1;32m    639\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m concrete_type \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m concrete_type_store\u001b[38;5;241m.\u001b[39mmethods_compiled:\n\u001b[0;32m--> 640\u001b[0m     \u001b[43mcreate_methods_and_properties_from_stubs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    641\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconcrete_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod_stubs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproperty_stubs\u001b[49m\n\u001b[1;32m    642\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    643\u001b[0m     \u001b[38;5;66;03m# Create hooks after methods to ensure no name collisions between hooks and methods.\u001b[39;00m\n\u001b[1;32m    644\u001b[0m     \u001b[38;5;66;03m# If done before, hooks can overshadow methods that aren't exported.\u001b[39;00m\n\u001b[1;32m    645\u001b[0m     create_hooks_from_stubs(concrete_type, hook_stubs, pre_hook_stubs)\n",
      "File \u001b[0;32m~/miniconda3/envs/ml2/lib/python3.11/site-packages/torch/jit/_recursive.py:474\u001b[0m, in \u001b[0;36mcreate_methods_and_properties_from_stubs\u001b[0;34m(concrete_type, method_stubs, property_stubs)\u001b[0m\n\u001b[1;32m    472\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e: \n\u001b[1;32m    473\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcaca\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 474\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "File \u001b[0;32m~/miniconda3/envs/ml2/lib/python3.11/site-packages/torch/jit/_recursive.py:469\u001b[0m, in \u001b[0;36mcreate_methods_and_properties_from_stubs\u001b[0;34m(concrete_type, method_stubs, property_stubs)\u001b[0m\n\u001b[1;32m    466\u001b[0m property_rcbs \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mresolution_callback \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m property_stubs]\n\u001b[1;32m    468\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 469\u001b[0m     \u001b[43mconcrete_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_methods_and_properties\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    470\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproperty_defs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproperty_rcbs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod_defs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod_rcbs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod_defaults\u001b[49m\n\u001b[1;32m    471\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    472\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e: \n\u001b[1;32m    473\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcaca\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/ml2/lib/python3.11/site-packages/torch/jit/_recursive.py:1041\u001b[0m, in \u001b[0;36mcompile_unbound_method\u001b[0;34m(concrete_type, fn)\u001b[0m\n\u001b[1;32m   1037\u001b[0m stub \u001b[38;5;241m=\u001b[39m make_stub(fn, fn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m   1038\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_jit_internal\u001b[38;5;241m.\u001b[39m_disable_emit_hooks():\n\u001b[1;32m   1039\u001b[0m     \u001b[38;5;66;03m# We don't want to call the hooks here since the graph that is calling\u001b[39;00m\n\u001b[1;32m   1040\u001b[0m     \u001b[38;5;66;03m# this function is not yet complete\u001b[39;00m\n\u001b[0;32m-> 1041\u001b[0m     \u001b[43mcreate_methods_and_properties_from_stubs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconcrete_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mstub\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1042\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m stub\n",
      "File \u001b[0;32m~/miniconda3/envs/ml2/lib/python3.11/site-packages/torch/jit/_recursive.py:474\u001b[0m, in \u001b[0;36mcreate_methods_and_properties_from_stubs\u001b[0;34m(concrete_type, method_stubs, property_stubs)\u001b[0m\n\u001b[1;32m    472\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e: \n\u001b[1;32m    473\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcaca\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 474\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "File \u001b[0;32m~/miniconda3/envs/ml2/lib/python3.11/site-packages/torch/jit/_recursive.py:469\u001b[0m, in \u001b[0;36mcreate_methods_and_properties_from_stubs\u001b[0;34m(concrete_type, method_stubs, property_stubs)\u001b[0m\n\u001b[1;32m    466\u001b[0m property_rcbs \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mresolution_callback \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m property_stubs]\n\u001b[1;32m    468\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 469\u001b[0m     \u001b[43mconcrete_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_methods_and_properties\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    470\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproperty_defs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproperty_rcbs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod_defs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod_rcbs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod_defaults\u001b[49m\n\u001b[1;32m    471\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    472\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e: \n\u001b[1;32m    473\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcaca\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/ml2/lib/python3.11/site-packages/torch/jit/_recursive.py:1041\u001b[0m, in \u001b[0;36mcompile_unbound_method\u001b[0;34m(concrete_type, fn)\u001b[0m\n\u001b[1;32m   1037\u001b[0m stub \u001b[38;5;241m=\u001b[39m make_stub(fn, fn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m   1038\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_jit_internal\u001b[38;5;241m.\u001b[39m_disable_emit_hooks():\n\u001b[1;32m   1039\u001b[0m     \u001b[38;5;66;03m# We don't want to call the hooks here since the graph that is calling\u001b[39;00m\n\u001b[1;32m   1040\u001b[0m     \u001b[38;5;66;03m# this function is not yet complete\u001b[39;00m\n\u001b[0;32m-> 1041\u001b[0m     \u001b[43mcreate_methods_and_properties_from_stubs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconcrete_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mstub\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1042\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m stub\n",
      "File \u001b[0;32m~/miniconda3/envs/ml2/lib/python3.11/site-packages/torch/jit/_recursive.py:474\u001b[0m, in \u001b[0;36mcreate_methods_and_properties_from_stubs\u001b[0;34m(concrete_type, method_stubs, property_stubs)\u001b[0m\n\u001b[1;32m    472\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e: \n\u001b[1;32m    473\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcaca\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 474\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "File \u001b[0;32m~/miniconda3/envs/ml2/lib/python3.11/site-packages/torch/jit/_recursive.py:469\u001b[0m, in \u001b[0;36mcreate_methods_and_properties_from_stubs\u001b[0;34m(concrete_type, method_stubs, property_stubs)\u001b[0m\n\u001b[1;32m    466\u001b[0m property_rcbs \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mresolution_callback \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m property_stubs]\n\u001b[1;32m    468\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 469\u001b[0m     \u001b[43mconcrete_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_methods_and_properties\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    470\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproperty_defs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproperty_rcbs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod_defs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod_rcbs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod_defaults\u001b[49m\n\u001b[1;32m    471\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    472\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e: \n\u001b[1;32m    473\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcaca\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/ml2/lib/python3.11/site-packages/torch/jit/_recursive.py:1041\u001b[0m, in \u001b[0;36mcompile_unbound_method\u001b[0;34m(concrete_type, fn)\u001b[0m\n\u001b[1;32m   1037\u001b[0m stub \u001b[38;5;241m=\u001b[39m make_stub(fn, fn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m   1038\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_jit_internal\u001b[38;5;241m.\u001b[39m_disable_emit_hooks():\n\u001b[1;32m   1039\u001b[0m     \u001b[38;5;66;03m# We don't want to call the hooks here since the graph that is calling\u001b[39;00m\n\u001b[1;32m   1040\u001b[0m     \u001b[38;5;66;03m# this function is not yet complete\u001b[39;00m\n\u001b[0;32m-> 1041\u001b[0m     \u001b[43mcreate_methods_and_properties_from_stubs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconcrete_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mstub\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1042\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m stub\n",
      "File \u001b[0;32m~/miniconda3/envs/ml2/lib/python3.11/site-packages/torch/jit/_recursive.py:474\u001b[0m, in \u001b[0;36mcreate_methods_and_properties_from_stubs\u001b[0;34m(concrete_type, method_stubs, property_stubs)\u001b[0m\n\u001b[1;32m    472\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e: \n\u001b[1;32m    473\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcaca\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 474\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "File \u001b[0;32m~/miniconda3/envs/ml2/lib/python3.11/site-packages/torch/jit/_recursive.py:469\u001b[0m, in \u001b[0;36mcreate_methods_and_properties_from_stubs\u001b[0;34m(concrete_type, method_stubs, property_stubs)\u001b[0m\n\u001b[1;32m    466\u001b[0m property_rcbs \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mresolution_callback \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m property_stubs]\n\u001b[1;32m    468\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 469\u001b[0m     \u001b[43mconcrete_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_methods_and_properties\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    470\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproperty_defs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproperty_rcbs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod_defs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod_rcbs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod_defaults\u001b[49m\n\u001b[1;32m    471\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    472\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e: \n\u001b[1;32m    473\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcaca\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: \nArguments for call are not valid.\nThe following variants are available:\n  \n  aten::mul.Tensor(Tensor self, Tensor other) -> Tensor:\n  Expected a value of type 'Tensor' for argument 'self' but instead found type 'Optional[Tensor]'.\n  \n  aten::mul.Scalar(Tensor self, Scalar other) -> Tensor:\n  Expected a value of type 'Tensor' for argument 'self' but instead found type 'Optional[Tensor]'.\n  \n  aten::mul.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!):\n  Expected a value of type 'Tensor' for argument 'self' but instead found type 'Optional[Tensor]'.\n  \n  aten::mul.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!):\n  Expected a value of type 'Tensor' for argument 'self' but instead found type 'Optional[Tensor]'.\n  \n  aten::mul.left_t(t[] l, int n) -> t[]:\n  Could not match type Optional[Tensor] to List[t] in argument 'l': Cannot match List[t] to Optional[Tensor].\n  \n  aten::mul.right_(int n, t[] l) -> t[]:\n  Expected a value of type 'int' for argument 'n' but instead found type 'Optional[Tensor]'.\n  \n  aten::mul.int(int a, int b) -> int:\n  Expected a value of type 'int' for argument 'a' but instead found type 'Optional[Tensor]'.\n  \n  aten::mul.complex(complex a, complex b) -> complex:\n  Expected a value of type 'complex' for argument 'a' but instead found type 'Optional[Tensor]'.\n  \n  aten::mul.float(float a, float b) -> float:\n  Expected a value of type 'float' for argument 'a' but instead found type 'Optional[Tensor]'.\n  \n  aten::mul.int_complex(int a, complex b) -> complex:\n  Expected a value of type 'int' for argument 'a' but instead found type 'Optional[Tensor]'.\n  \n  aten::mul.complex_int(complex a, int b) -> complex:\n  Expected a value of type 'complex' for argument 'a' but instead found type 'Optional[Tensor]'.\n  \n  aten::mul.float_complex(float a, complex b) -> complex:\n  Expected a value of type 'float' for argument 'a' but instead found type 'Optional[Tensor]'.\n  \n  aten::mul.complex_float(complex a, float b) -> complex:\n  Expected a value of type 'complex' for argument 'a' but instead found type 'Optional[Tensor]'.\n  \n  aten::mul.int_float(int a, float b) -> float:\n  Expected a value of type 'int' for argument 'a' but instead found type 'Optional[Tensor]'.\n  \n  aten::mul.float_int(float a, int b) -> float:\n  Expected a value of type 'float' for argument 'a' but instead found type 'Optional[Tensor]'.\n  \n  aten::mul(Scalar a, Scalar b) -> Scalar:\n  Expected a value of type 'number' for argument 'a' but instead found type 'Optional[Tensor]'.\n  \n  mul(float a, Tensor b) -> Tensor:\n  Expected a value of type 'float' for argument 'a' but instead found type 'Optional[Tensor]'.\n  \n  mul(int a, Tensor b) -> Tensor:\n  Expected a value of type 'int' for argument 'a' but instead found type 'Optional[Tensor]'.\n  \n  mul(complex a, Tensor b) -> Tensor:\n  Expected a value of type 'complex' for argument 'a' but instead found type 'Optional[Tensor]'.\n\nThe original call is:\n  File \"/var/folders/vk/nn1706pd25b57gxz9y3p1wqh0000gn/T/ipykernel_45941/43206288.py\", line 31\n        - cache : (optional): callbacks cache the original parameter value for dynamical modification\n        \"\"\"\n        param.set_(cache * cache)\n                   ~~~~~~~~~~~~~ <--- HERE\n'Square.apply_to_param' is being compiled since it was called from 'Square.apply'\n  File \"/Users/domkirke/Dropbox/code/torchbend/docs/../torchbend/bending/base.py\", line 168\n        for i, v in enumerate(self._bending_targets):\n            v_cached = self.cache_from_id(i).data\n            self.apply_to_param(i, v, v_cached)\n            ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE\n'Square.apply' is being compiled since it was called from 'ScriptedBendedModule._update_weights'\n  File \"/Users/domkirke/Dropbox/code/torchbend/docs/../torchbend/tracing/script.py\", line 150\n            for i, c in enumerate(self._bending_callbacks):\n                for j in callbacks:\n                    if i == j: c.apply()\n                               ~~~~~~~ <--- HERE\n'ScriptedBendedModule._update_weights' is being compiled since it was called from 'ScriptedBendedModule._set_bending_control'\n  File \"/Users/domkirke/Dropbox/code/torchbend/docs/../torchbend/tracing/script.py\", line 167\n            if v.name == name:\n                v.set_value(value)\n        self._update_weights(name)\n        ~~~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE\n        return 0\n"
     ]
    }
   ],
   "source": [
    "import torch, torch.nn as nn\n",
    "from typing import Optional\n",
    "import sys; sys.path.append(\"..\")\n",
    "import torchbend as tb\n",
    "\n",
    "class Greg(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv_module_1 = nn.Conv1d(1, 4, 3)\n",
    "        self.conv_module_2 = nn.Conv1d(4, 1, 3)\n",
    "    def forward(self, x):\n",
    "        out_1 = self.conv_module_1(x)\n",
    "        out_2 = self.conv_module_2(out_1)\n",
    "        return out_2\n",
    "    \n",
    "class Square(tb.BendingCallback):\n",
    "    # define class attributes to provide meta information on what the callback can do\n",
    "    weight_compatible = True \n",
    "    activation_compatible = True\n",
    "    jit_compatible = True \n",
    "    nntilde_compatible = True \n",
    "\n",
    "    def apply_to_param(self, idx: int, param: nn.Parameter, cache: Optional[torch.Tensor]):\n",
    "        \"\"\"\n",
    "        for weight bending, apply_to_param receives three arguments : \n",
    "        - idx of recorded param (int) : bended parameters are recorded as lists to be jit-compatible, such that providing the id is useful to recover \n",
    "        paramter-wise buffers\n",
    "        - param : the module parameter, modified in place\n",
    "        - cache : (optional): callbacks cache the original parameter value for dynamical modification\n",
    "        \"\"\"\n",
    "        if cache is not None:\n",
    "            param.set_(cache * cache)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, name: Optional[str] = None):\n",
    "        \"\"\"\n",
    "        for activation bending, apply_to_param receives two arguments : \n",
    "        - x : value of activation\n",
    "        - name : the activation name, that can be used to retrieve activation-wise buffers\n",
    "        \"\"\"\n",
    "        return x * x\n",
    "\n",
    "\n",
    "module = Greg()\n",
    "bended = tb.BendedModule(module)\n",
    "x = torch.randn(1, 1, 32)\n",
    "_, out = bended.trace(x=x, _return_out=True)\n",
    "print(out[0])\n",
    "\n",
    "bended.bend(Square(), \"conv_module_1$\", \"conv_module.1.weight\")\n",
    "out = bended(x)\n",
    "print(out)\n",
    "\n",
    "scripted = bended.script()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stateful callbacks \n",
    "\n",
    "Stateful callbacks may be a little trickier, nothing heavy do not worry but some steps can be cumbersome due to torch.jit. Stateful callbacks implies overriding some more methods :\n",
    "- `register_parameter` : register a bended parameter in the callback module\n",
    "- `register_activation` : register a bended activation shape in the callback module\n",
    "- `update`, that updates inner states after a `BendingParameter` change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.3871,  0.3391, -0.0650,  0.0677,  0.4748, -0.0197, -0.0200,\n",
      "           0.4790,  0.6838, -0.0125, -0.1180,  0.0709,  0.4235,  0.3217,\n",
      "           0.3106,  0.1544,  0.1873,  0.1464, -0.4558,  0.0875,  0.7703,\n",
      "           0.3424, -0.0955,  0.1810,  0.6059,  0.0449,  0.0875,  0.5416]]],\n",
      "       grad_fn=<ConvolutionBackward0>)\n",
      "tensor([[[ 0.3871,  0.3391, -0.0650,  0.0677,  0.4748, -0.0197, -0.0200,\n",
      "           0.4790,  0.6838, -0.0125, -0.1180,  0.0709,  0.4235,  0.3217,\n",
      "           0.3106,  0.1544,  0.1873,  0.1464, -0.4558,  0.0875,  0.7703,\n",
      "           0.3424, -0.0955,  0.1810,  0.6059,  0.0449,  0.0875,  0.5416]]],\n",
      "       grad_fn=<MulBackward0>)\n",
      "tensor([[[ 0.0000,  0.0000, -0.1217, -0.0214,  0.0000,  0.0000, -0.1343,\n",
      "           0.0000,  0.0000,  0.0000, -0.3166, -0.0575,  0.0000,  0.0000,\n",
      "           0.0000,  0.0000,  0.0000,  0.0000, -0.6491, -0.0991,  0.0000,\n",
      "           0.0000, -0.1503,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]]],\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "class UpClamp(tb.BendingCallback):\n",
    "    weight_compatible = True \n",
    "    activation_compatible = True\n",
    "    jit_compatible = True \n",
    "    nntilde_compatible = True \n",
    "\n",
    "    # provide this to inform BendingCallback that this init arg can be contorlled\n",
    "    controllable_params = ['threshold']\n",
    "\n",
    "    def __init__(self, threshold = 1.):\n",
    "        super().__init__()\n",
    "        self.threshold = threshold\n",
    "        self._masks = torch.nn.ParameterList()\n",
    "        self._mask_names = []\n",
    "\n",
    "    def _add_mask(self, name, parameter):\n",
    "        mask = nn.Parameter((parameter.data > self.get('threshold')).float(), requires_grad=False)\n",
    "        self._masks.append(mask)\n",
    "        # disable gradient\n",
    "        self._mask_names.append(name)\n",
    "\n",
    "    def register_parameter(self, parameter, name=None, cache = True):\n",
    "        name = super().register_parameter(parameter, name=name, cache=cache)\n",
    "        self._add_mask(name, parameter=parameter)\n",
    "\n",
    "    def register_activation(self, name, shape):\n",
    "        name = super().register_activation(name, shape)\n",
    "        # here we don't need to do anything, as only parameter updates require states\n",
    "        return name\n",
    "    \n",
    "    def get_mask_from_index(self, idx: int):\n",
    "        # Don't judge me, this is because torch.jit only allows literal indexing. \n",
    "        for i, v in enumerate(self._masks):\n",
    "            if i == idx:\n",
    "                return v\n",
    "        raise tb.BendingCallbackException('%d not present in masks'%idx)\n",
    "    \n",
    "    def update(self):\n",
    "        for i, v in enumerate(self._masks):\n",
    "                v.data.set_((self.get_cache(i) > self.get('threshold')).float())\n",
    "\n",
    "    def apply_to_param(self, idx: int, param: nn.Parameter, cache: Optional[torch.Tensor]):\n",
    "        \"\"\"\n",
    "        for weight bending, apply_to_param receives three arguments : \n",
    "        - idx of recorded param (int) : bended parameters are recorded as lists to be jit-compatible, such that providing the id is useful to recover \n",
    "        paramter-wise buffers\n",
    "        - param : the module parameter, modified in place\n",
    "        - cache : (optional): callbacks cache the original parameter value for dynamical modification\n",
    "        \"\"\"\n",
    "        assert cache is not None\n",
    "        param.set_(cache * self.get_mask_from_index(idx))\n",
    "\n",
    "    def forward(self, x: torch.Tensor, name: Optional[str] = None):\n",
    "        \"\"\"\n",
    "        for activation bending, apply_to_param receives two arguments : \n",
    "        - x : value of activation\n",
    "        - name : the activation name, that can be used to retrieve activation-wise buffers\n",
    "        \"\"\"\n",
    "        return x * (x < self.get('threshold')).float()\n",
    "\n",
    "module = Greg()\n",
    "bended = tb.BendedModule(module)\n",
    "x = torch.randn(1, 1, 32)\n",
    "_, out = bended.trace(x=x, _return_out=True)\n",
    "print(out[0])\n",
    "\n",
    "c1 = tb.BendingParameter('threshold', value=1.)\n",
    "bended.bend(UpClamp(threshold=c1), \"conv_module_2$\", \"conv_module_1.weight\")\n",
    "out = bended(x)\n",
    "print(out)\n",
    "\n",
    "bended.update(c1.name, 0.)\n",
    "out = bended(x)\n",
    "print(out)\n",
    "\n",
    "scripted = bended.script()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
