{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining callbacks\n",
    "\n",
    "\n",
    "We will see in this tutorial how to define a bending callaback to perform custom callback operations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining stateless callbacks\n",
    "\n",
    "To start easy, let's define a callback that does not need any internal buffer. A bending callback must define two functions : \n",
    "- the `forward` function, that is used for activation bending\n",
    "- the `apply_to_param` function, that is used for weight bending\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.3480, -0.0668, -1.1553,  0.6916, -1.0866, -0.2815,  0.0194,\n",
      "          -0.7783, -0.3196, -0.6542,  0.0686, -0.9633, -0.2639, -0.4900,\n",
      "          -0.8211,  0.0595, -0.4032, -0.9059,  0.1834, -1.1281,  0.1439,\n",
      "          -0.4905, -0.7331, -0.4273,  0.0104, -0.9659,  0.2734, -1.0282]]],\n",
      "       grad_fn=<ConvolutionBackward0>)\n",
      "tensor([[[-0.3753, -0.4296, -0.2166, -0.5104, -0.1630, -0.2979, -0.3271,\n",
      "          -0.1033, -0.2116, -0.1745,  0.0533, -0.1424, -0.1964, -0.1858,\n",
      "          -0.2004, -0.2497, -0.2142, -0.1430, -0.2445, -0.1591, -0.1478,\n",
      "          -0.2582, -0.1512, -0.3252, -0.3273, -0.1801, -0.3254, -0.1487]]],\n",
      "       grad_fn=<ConvolutionBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch, torch.nn as nn\n",
    "from typing import Optional\n",
    "import sys; sys.path.append(\"..\")\n",
    "import torchbend as tb\n",
    "\n",
    "class Greg(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv_module_1 = nn.Conv1d(1, 4, 3)\n",
    "        self.conv_module_2 = nn.Conv1d(4, 1, 3)\n",
    "    def forward(self, x):\n",
    "        out_1 = self.conv_module_1(x)\n",
    "        out_2 = self.conv_module_2(out_1)\n",
    "        return out_2\n",
    "    \n",
    "class Square(tb.BendingCallback):\n",
    "    # define class attributes to provide meta information on what the callback can do\n",
    "    weight_compatible = True \n",
    "    activation_compatible = True\n",
    "    jit_compatible = True \n",
    "    nntilde_compatible = True \n",
    "\n",
    "    def apply_to_param(self, idx: int, param: nn.Parameter, cache: Optional[torch.Tensor]):\n",
    "        \"\"\"\n",
    "        for weight bending, apply_to_param receives three arguments : \n",
    "        - idx of recorded param (int) : bended parameters are recorded as lists to be jit-compatible, such that providing the id is useful to recover \n",
    "        paramter-wise buffers\n",
    "        - param : the module parameter, modified in place\n",
    "        - cache : (optional): callbacks cache the original parameter value for dynamical modification\n",
    "        \"\"\"\n",
    "        param.set_(cache * cache)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, name: Optional[str] = None):\n",
    "        \"\"\"\n",
    "        for activation bending, apply_to_param receives two arguments : \n",
    "        - x : value of activation\n",
    "        - name : the activation name, that can be used to retrieve activation-wise buffers\n",
    "        \"\"\"\n",
    "        return x * x\n",
    "\n",
    "\n",
    "module = Greg()\n",
    "bended = tb.BendedModule(module)\n",
    "x = torch.randn(1, 1, 32)\n",
    "_, out = bended.trace(x=x, _return_out=True)\n",
    "print(out[0])\n",
    "\n",
    "bended.bend(Square(), \"conv_module_1$\", \"conv_module.1.weight\")\n",
    "out = bended(x)\n",
    "print(out)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stateful callbacks \n",
    "\n",
    "Stateful callbacks may be a little trickier, nothing heavy do not worry but some steps can be cumbersome due to torch.jit. Stateful callbacks implies overriding some more methods :\n",
    "- `register_parameter` : register a bended parameter in the callback module\n",
    "- `register_activation` : register a bended activation shape in the callback module\n",
    "- `update`, that updates inner states after a `BendingParameter` change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.2619,  0.1348,  0.3441,  0.4760,  0.6323,  0.5979,  0.7836,\n",
      "           0.3744, -0.0860, -0.3237,  0.1679,  0.4835,  0.9399,  0.3376,\n",
      "           0.2188,  0.0457,  0.4031,  0.6875,  0.5118,  0.2731,  0.0115,\n",
      "           0.2652,  0.1423,  0.2137,  0.0537,  0.2528,  0.1761,  0.4793]]],\n",
      "       grad_fn=<ConvolutionBackward0>)\n",
      "tensor([[[-0.2619,  0.1348,  0.3441,  0.4760,  0.6323,  0.5979,  0.7836,\n",
      "           0.3744, -0.0860, -0.3237,  0.1679,  0.4835,  0.9399,  0.3376,\n",
      "           0.2188,  0.0457,  0.4031,  0.6875,  0.5118,  0.2731,  0.0115,\n",
      "           0.2652,  0.1423,  0.2137,  0.0537,  0.2528,  0.1761,  0.4793]]],\n",
      "       grad_fn=<MulBackward0>)\n",
      "tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0.]]], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "class UpClamp(tb.BendingCallback):\n",
    "    weight_compatible = True \n",
    "    activation_compatible = True\n",
    "    jit_compatible = True \n",
    "    nntilde_compatible = True \n",
    "\n",
    "    # provide this to inform BendingCallback that this init arg can be contorlled\n",
    "    controllable_params = ['threshold']\n",
    "\n",
    "    def __init__(self, threshold = 1.):\n",
    "        super().__init__()\n",
    "        self.threshold = threshold\n",
    "        self._masks = torch.nn.ParameterList()\n",
    "        self._mask_names = []\n",
    "\n",
    "    def _init_mask(self, parameter):\n",
    "        return nn.Parameter((parameter.data > self.get('threshold')).float().requires_grad_(False))\n",
    "\n",
    "    def _add_mask(self, name, parameter):\n",
    "        self._masks.append(self._init_mask(parameter))\n",
    "        # disable gradient\n",
    "        self._mask_names.append(name)\n",
    "\n",
    "    def register_parameter(self, parameter, name=None, cache = True):\n",
    "        name = super().register_parameter(parameter, name=name, cache=cache)\n",
    "        self._add_mask(name, parameter=parameter)\n",
    "\n",
    "    def register_activation(self, name, shape):\n",
    "        name = super().register_activation(name, shape)\n",
    "        # here we don't need to do anything, as only parameter updates require states\n",
    "        return name\n",
    "    \n",
    "    def get_mask_from_index(self, idx: int):\n",
    "        # Don't judge me, this is because torch.jit only allows literal indexing. \n",
    "        for i, v in enumerate(self._masks):\n",
    "            if i == idx:\n",
    "                return v\n",
    "        raise tb.BendingCallbackException('%d not present in masks'%idx)\n",
    "    \n",
    "    def update(self):\n",
    "        with torch.set_grad_enabled(False):\n",
    "            for i, v in enumerate(self._masks):\n",
    "                v.set_(self._init_mask(self.get_cache(i)))\n",
    "\n",
    "    def apply_to_param(self, idx: int, param: nn.Parameter, cache: Optional[torch.Tensor]):\n",
    "        \"\"\"\n",
    "        for weight bending, apply_to_param receives three arguments : \n",
    "        - idx of recorded param (int) : bended parameters are recorded as lists to be jit-compatible, such that providing the id is useful to recover \n",
    "        paramter-wise buffers\n",
    "        - param : the module parameter, modified in place\n",
    "        - cache : (optional): callbacks cache the original parameter value for dynamical modification\n",
    "        \"\"\"\n",
    "        assert cache is not None\n",
    "        param.set_(cache * self.get_mask_from_index(idx))\n",
    "\n",
    "    def forward(self, x: torch.Tensor, name: Optional[str] = None):\n",
    "        \"\"\"\n",
    "        for activation bending, apply_to_param receives two arguments : \n",
    "        - x : value of activation\n",
    "        - name : the activation name, that can be used to retrieve activation-wise buffers\n",
    "        \"\"\"\n",
    "        return x * (x < self.get('threshold')).float()\n",
    "\n",
    "\n",
    "\n",
    "module = Greg()\n",
    "bended = tb.BendedModule(module)\n",
    "x = torch.randn(1, 1, 32)\n",
    "_, out = bended.trace(x=x, _return_out=True)\n",
    "print(out[0])\n",
    "\n",
    "c1 = tb.BendingParameter('threshold', value=1.)\n",
    "bended.bend(UpClamp(threshold=c1), \"conv_module_2$\", \"conv_module_1.weight\")\n",
    "out = bended(x)\n",
    "print(out)\n",
    "\n",
    "bended.update(c1.name, 0.)\n",
    "out = bended(x)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
