{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining callbacks\n",
    "\n",
    "\n",
    "We will see in this tutorial how to define a bending callaback to perform custom callback operations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining stateless callbacks\n",
    "\n",
    "To start easy, let's define a callback that does not need any internal buffer. A bending callback must define two functions : \n",
    "- the `bend_input` function, that is used for weight and activation bending\n",
    "- the `apply_to_param` function, that is used for weight bending after JIT compilation \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.0384, -0.4096, -0.3645, -0.8078, -0.6729, -0.7594, -0.5992,\n",
      "           0.0806, -0.8036, -0.4242, -0.3870, -0.6751, -0.2078, -0.5525,\n",
      "          -0.2823, -0.5777, -0.4736, -0.0823, -0.7809, -0.8411, -0.2782,\n",
      "          -0.2959, -0.3533, -0.1839, -0.0553, -0.2795, -0.5492, -0.1855]]],\n",
      "       grad_fn=<ConvolutionBackward0>)\n",
      "tensor([[[-0.2223, -0.3371, -0.3487, -0.2570, -0.2490, -0.2163, -0.2379,\n",
      "          -0.3109, -0.2827, -0.2512, -0.2077, -0.2559, -0.2952, -0.2522,\n",
      "          -0.2119, -0.1845, -0.2836, -0.2968, -0.2521, -0.1654, -0.1012,\n",
      "          -0.0396, -0.0348, -0.0646, -0.1804, -0.2581, -0.2929, -0.2695]]],\n",
      "       grad_fn=<ConvolutionBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch, torch.nn as nn\n",
    "from typing import Optional\n",
    "import sys; sys.path.append(\"..\")\n",
    "import torchbend as tb\n",
    "\n",
    "class Greg(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv_module_1 = nn.Conv1d(1, 4, 3)\n",
    "        self.conv_module_2 = nn.Conv1d(4, 1, 3)\n",
    "    def forward(self, x):\n",
    "        out_1 = self.conv_module_1(x)\n",
    "        out_2 = self.conv_module_2(out_1)\n",
    "        return out_2\n",
    "    \n",
    "class Square(tb.BendingCallback):\n",
    "    # define class attributes to provide meta information on what the callback can do\n",
    "    weight_compatible = True \n",
    "    activation_compatible = True\n",
    "    jit_compatible = True \n",
    "    nntilde_compatible = True \n",
    "\n",
    "    def apply_to_param(self, idx: int, param: nn.Parameter, cache: Optional[torch.Tensor]):\n",
    "        \"\"\"\n",
    "        for weight bending, apply_to_param receives three arguments : \n",
    "        - idx of recorded param (int) : bended parameters are recorded as lists to be jit-compatible, such that providing the id is useful to recover \n",
    "        paramter-wise buffers\n",
    "        - param : the module parameter, modified in place\n",
    "        - cache : (optional): callbacks cache the original parameter value for dynamical modification\n",
    "        \"\"\"\n",
    "        if cache is not None:\n",
    "            param.set_(cache * cache)\n",
    "\n",
    "    def bend_input(self, x: torch.Tensor, name: Optional[str] = None):\n",
    "        \"\"\"\n",
    "        for activation bending, apply_to_param receives two arguments : \n",
    "        - x : value of activation\n",
    "        - name : the activation name, that can be used to retrieve activation-wise buffers\n",
    "        \"\"\"\n",
    "        return x * x\n",
    "\n",
    "\n",
    "module = Greg()\n",
    "bended = tb.BendedModule(module)\n",
    "x = torch.randn(1, 1, 32)\n",
    "_, out = bended.trace(x=x, _return_out=True)\n",
    "print(out[0])\n",
    "\n",
    "bended.bend(Square(), \"conv_module_1$\", \"conv_module.1.weight\")\n",
    "out = bended(x)\n",
    "print(out)\n",
    "\n",
    "scripted = bended.script()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stateful callbacks \n",
    "\n",
    "Stateful callbacks may be a little trickier, nothing heavy do not worry but some steps can be cumbersome due to torch.jit. Stateful callbacks implies overriding some more methods :\n",
    "- `register_parameter` : register a bended parameter in the callback module\n",
    "- `register_activation` : register a bended activation shape in the callback module\n",
    "- `update`, that updates inner states after a `BendingParameter` change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.5531,  0.2135,  0.1794, -0.1940, -0.2208,  0.0698,  0.2735,\n",
      "           0.1670,  0.3737,  0.2017,  0.4460,  0.3786,  0.5029,  0.3215,\n",
      "           0.3719,  0.0941,  0.2514, -0.1249, -0.0867, -0.2876, -0.3940,\n",
      "          -0.2336,  0.3189,  0.3808,  0.2921, -0.1922, -0.4330, -0.8138]]],\n",
      "       grad_fn=<ConvolutionBackward0>)\n",
      "tensor([[[ 0.5531,  0.2135,  0.1794, -0.1940, -0.2208,  0.0698,  0.2735,\n",
      "           0.1670,  0.3737,  0.2017,  0.4460,  0.3786,  0.5029,  0.3215,\n",
      "           0.3719,  0.0941,  0.2514, -0.1249, -0.0867, -0.2876, -0.3940,\n",
      "          -0.2336,  0.3189,  0.3808,  0.2921, -0.1922, -0.4330, -0.8138]]],\n",
      "       grad_fn=<MulBackward0>)\n",
      "tensor([[[ 0.0000,  0.0000,  0.0000, -0.3109, -0.2443,  0.0000,  0.0000,\n",
      "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "           0.0000,  0.0000,  0.0000, -0.2082, -0.2309, -0.4662, -0.4808,\n",
      "          -0.2194,  0.0000,  0.0000,  0.0000, -0.4393, -0.6785, -0.9573]]],\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "class UpClamp(tb.BendingCallback):\n",
    "    weight_compatible = True \n",
    "    activation_compatible = True\n",
    "    jit_compatible = True \n",
    "    nntilde_compatible = True \n",
    "\n",
    "    # provide this to inform BendingCallback that this init arg can be contorlled\n",
    "    controllable_params = ['threshold']\n",
    "\n",
    "    def __init__(self, threshold = 1.):\n",
    "        super().__init__()\n",
    "        self.threshold = threshold\n",
    "        self._masks = torch.nn.ParameterList()\n",
    "        self._mask_names = []\n",
    "\n",
    "    def _add_mask(self, name, parameter):\n",
    "        mask = nn.Parameter((parameter.data > self.get('threshold')).float(), requires_grad=False)\n",
    "        self._masks.append(mask)\n",
    "        # disable gradient\n",
    "        self._mask_names.append(name)\n",
    "\n",
    "    def register_parameter(self, parameter, name=None, cache = True):\n",
    "        name = super().register_parameter(parameter, name=name, cache=cache)\n",
    "        self._add_mask(name, parameter=parameter)\n",
    "\n",
    "    def register_activation(self, name, shape):\n",
    "        name = super().register_activation(name, shape)\n",
    "        # here we don't need to do anything, as only parameter updates require states\n",
    "        return name\n",
    "    \n",
    "    def get_mask_from_index(self, idx: int):\n",
    "        # Don't judge me, this is because torch.jit only allows literal indexing. \n",
    "        for i, v in enumerate(self._masks):\n",
    "            if i == idx:\n",
    "                return v\n",
    "        raise tb.BendingCallbackException('%d not present in masks'%idx)\n",
    "    \n",
    "    def update(self):\n",
    "        for i, v in enumerate(self._masks):\n",
    "                v.data.set_((self.get_cache(i) > self.get('threshold')).float())\n",
    "\n",
    "    def apply_to_param(self, idx: int, param: nn.Parameter, cache: Optional[torch.Tensor]):\n",
    "        \"\"\"\n",
    "        for weight bending, apply_to_param receives three arguments : \n",
    "        - idx of recorded param (int) : bended parameters are recorded as lists to be jit-compatible, such that providing the id is useful to recover \n",
    "        paramter-wise buffers\n",
    "        - param : the module parameter, modified in place\n",
    "        - cache : (optional): callbacks cache the original parameter value for dynamical modification\n",
    "        \"\"\"\n",
    "        assert cache is not None\n",
    "        param.set_(cache * self.get_mask_from_index(idx))\n",
    "\n",
    "    def bend_input(self, x: torch.Tensor, name: Optional[str] = None):\n",
    "        \"\"\"\n",
    "        for activation bending, apply_to_param receives two arguments : \n",
    "        - x : value of activation\n",
    "        - name : the activation name, that can be used to retrieve activation-wise buffers\n",
    "        \"\"\"\n",
    "        return x * (x < self.get('threshold')).float()\n",
    "\n",
    "module = Greg()\n",
    "bended = tb.BendedModule(module)\n",
    "x = torch.randn(1, 1, 32)\n",
    "_, out = bended.trace(x=x, _return_out=True)\n",
    "print(out[0])\n",
    "\n",
    "c1 = tb.BendingParameter('threshold', value=1.)\n",
    "bended.bend(UpClamp(threshold=c1), \"conv_module_2$\", \"conv_module_1.weight\")\n",
    "out = bended(x)\n",
    "print(out)\n",
    "\n",
    "bended.update(c1.name, 0.)\n",
    "out = bended(x)\n",
    "print(out)\n",
    "\n",
    "scripted = bended.script()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Capturing\n",
    "\n",
    "Callbacks can also capture a set of entries, and perform some operations for inference. The most basic callback implementing that is the `tb.Capture` callback, that be can be used as a base class for every class implementing capturing : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "captured activation shape :  torch.Size([4, 4, 30])\n"
     ]
    }
   ],
   "source": [
    "module = Greg()\n",
    "bended = tb.BendedModule(module)\n",
    "x = torch.randn(1, 1, 32)\n",
    "_, out = bended.trace(x=x, _return_out=True)\n",
    "\n",
    "capture = tb.Capture()\n",
    "bended.bend(capture, 'conv_module_1$')\n",
    "\n",
    "capture.capture()\n",
    "for i in range(4):\n",
    "    bended(torch.randn(1, 1, 32))\n",
    "capture.stop()\n",
    "\n",
    "print(\"captured activation shape : \", capture.captures['conv_module_1'].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "captured activation shape :  torch.Size([4, 4, 30])\n",
      "captured activation shape :  torch.Size([8, 1, 28])\n"
     ]
    }
   ],
   "source": [
    "module = Greg()\n",
    "bended = tb.BendedModule(module)\n",
    "x = torch.randn(1, 1, 32)\n",
    "_, out = bended.trace(x=x, _return_out=True)\n",
    "\n",
    "c1 = tb.Capture()\n",
    "c2 = tb.Capture()\n",
    "bended.bend(c1, 'conv_module_1$')\n",
    "bended.bend(c2, 'conv_module_2$')\n",
    "\n",
    "# you can put arguments inside capture to specify some precise callbacks\n",
    "with bended.capture():\n",
    "    for i in range(4):\n",
    "        bended(torch.randn(1, 1, 32))\n",
    "\n",
    "# you can put arguments inside capture to specify some precise callbacks\n",
    "with bended.capture(c2):\n",
    "    for i in range(4):\n",
    "        bended(torch.randn(1, 1, 32))\n",
    "\n",
    "\n",
    "print(\"captured activation shape : \", c1.captures['conv_module_1'].shape)\n",
    "print(\"captured activation shape : \", c2.captures['conv_module_2'].shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can overload this `tb.Capture` to implement various callbacks based on recorded activations. For example, here we will remove half of the channels that have the lowest amplitude :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "torch.cat(): expected a non-empty list of Tensors",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 38\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m4\u001b[39m):\n\u001b[0;32m---> 38\u001b[0m     \u001b[43mbended\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Dropbox/code/torchbend/docs/../torchbend/tracing/module.py:337\u001b[0m, in \u001b[0;36mBendedModule.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    336\u001b[0m graph_module \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfx\u001b[38;5;241m.\u001b[39mGraphModule(module, graph)\n\u001b[0;32m--> 337\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgraph_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ml2/lib/python3.11/site-packages/torch/fx/graph_module.py:738\u001b[0m, in \u001b[0;36mGraphModule.recompile.<locals>.call_wrapped\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    737\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_wrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 738\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_wrapped_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ml2/lib/python3.11/site-packages/torch/fx/graph_module.py:316\u001b[0m, in \u001b[0;36m_WrappedCall.__call__\u001b[0;34m(self, obj, *args, **kwargs)\u001b[0m\n\u001b[1;32m    315\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 316\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "File \u001b[0;32m~/miniconda3/envs/ml2/lib/python3.11/site-packages/torch/fx/graph_module.py:303\u001b[0m, in \u001b[0;36m_WrappedCall.__call__\u001b[0;34m(self, obj, *args, **kwargs)\u001b[0m\n\u001b[1;32m    302\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 303\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcls\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    304\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/miniconda3/envs/ml2/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ml2/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m<eval_with_key>.17:6\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m      5\u001b[0m conv_module_1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv_module_1(x);  x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m conv_module_1_bended \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv_module_1_callback\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconv_module_1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mconv_module_1\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m;  conv_module_1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m      7\u001b[0m conv_module_2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv_module_2(conv_module_1_bended);  conv_module_1_bended \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/ml2/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ml2/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/Dropbox/code/torchbend/docs/../torchbend/bending/base.py:258\u001b[0m, in \u001b[0;36mCallbackChain.forward\u001b[0;34m(self, x, name)\u001b[0m\n\u001b[1;32m    257\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks):\n\u001b[0;32m--> 258\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/miniconda3/envs/ml2/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ml2/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "Cell \u001b[0;32mIn[6], line 25\u001b[0m, in \u001b[0;36mRemoveLowestHalf.forward\u001b[0;34m(self, x, name)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m: \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m()\n\u001b[0;32m---> 25\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x \u001b[38;5;241m*\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_mask_from_name\u001b[49m(name)\n",
      "File \u001b[0;32m~/miniconda3/envs/ml2/lib/python3.11/site-packages/torch/nn/modules/module.py:1729\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1728\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1729\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'RemoveLowestHalf' object has no attribute 'get_mask_from_name'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 36\u001b[0m\n\u001b[1;32m     33\u001b[0m c1 \u001b[38;5;241m=\u001b[39m RemoveLowestHalf()\n\u001b[1;32m     34\u001b[0m bended\u001b[38;5;241m.\u001b[39mbend(c1, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconv_module_1$\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 36\u001b[0m \u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbended\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcapture\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbended\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Dropbox/code/torchbend/docs/../torchbend/tracing/module.py:82\u001b[0m, in \u001b[0;36mBendedModuleCaptureContext.__exit__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__exit__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs):\n\u001b[0;32m---> 82\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_module\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdisable_capture\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_callbacks\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Dropbox/code/torchbend/docs/../torchbend/tracing/module.py:663\u001b[0m, in \u001b[0;36mBendedModule.disable_capture\u001b[0;34m(self, *callbacks)\u001b[0m\n\u001b[1;32m    661\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(callbacks) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m: callbacks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bending_callbacks\n\u001b[1;32m    662\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m callbacks:\n\u001b[0;32m--> 663\u001b[0m     \u001b[43mc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[6], line 20\u001b[0m, in \u001b[0;36mRemoveLowestHalf.stop\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstop\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m     19\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"overload the stop method to preprocess the captured elements\"\"\"\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_masks()\n",
      "File \u001b[0;32m~/Dropbox/code/torchbend/docs/../torchbend/bending/capture.py:48\u001b[0m, in \u001b[0;36mstop\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstop\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;66;03m#TODO make batched and non-batched version\u001b[39;00m\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_buffer_tmp\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m---> 48\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_captures:\n\u001b[1;32m     49\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_captures[k] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_concatenate_buffers([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_captures[k]] \u001b[38;5;241m+\u001b[39m v)\n\u001b[1;32m     50\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/Dropbox/code/torchbend/docs/../torchbend/bending/capture.py:20\u001b[0m, in \u001b[0;36m_concatenate_buffers\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_concatenate_buffers\u001b[39m(\u001b[38;5;28mself\u001b[39m, x): \n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcat(x, \u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m---> 20\u001b[0m \u001b[38;5;129m@property\u001b[39m \n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcaptures\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_captures\n\u001b[1;32m     24\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mis_ready\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: torch.cat(): expected a non-empty list of Tensors"
     ]
    }
   ],
   "source": [
    "class RemoveLowestHalf(tb.Capture):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self._not_ready_str = \"Please capture some activations before using this callback in inference mode\"\n",
    "        self.init_masks()\n",
    "\n",
    "    def init_masks(self):\n",
    "        self.masks = nn.ParameterDict()\n",
    "    \n",
    "    def compute_masks(self):\n",
    "        assert self._is_initialized\n",
    "        self.masks = nn.ParameterDict()\n",
    "        for k, v in self._captures.items():\n",
    "            energies = v.mean(dim=(0, 2)).abs()\n",
    "            threshold = torch.median(energies)\n",
    "            self.masks[k] = (energies > threshold)[None, :, None].float()\n",
    "\n",
    "    def stop(self):\n",
    "        \"\"\"overload the stop method to preprocess the captured elements\"\"\"\n",
    "        super().stop()\n",
    "        self.compute_masks()\n",
    "\n",
    "    def forward(self, x, name):\n",
    "        if name is None: raise Exception()\n",
    "        return x * self.get_mask_from_name(name)\n",
    "\n",
    "\n",
    "module = Greg()\n",
    "bended = tb.BendedModule(module)\n",
    "x = torch.randn(1, 1, 32)\n",
    "_, out = bended.trace(x=x, _return_out=True)\n",
    "\n",
    "c1 = RemoveLowestHalf()\n",
    "bended.bend(c1, 'conv_module_1$')\n",
    "\n",
    "with bended.capture():\n",
    "    for i in range(4):\n",
    "        bended(torch.randn(1, 1, 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
