{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining callbacks\n",
    "\n",
    "\n",
    "We will see in this tutorial how to define a bending callaback to perform custom callback operations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining stateless callbacks\n",
    "\n",
    "To start easy, let's define a callback that does not need any internal buffer. A bending callback must define two functions : \n",
    "- the `forward` function, that is used for activation bending\n",
    "- the `apply_to_param` function, that is used for weight bending\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.4191,  0.3484, -0.3344, -0.4367, -0.2221,  0.1707, -0.0082,\n",
      "          -0.1306, -0.0400, -0.3263, -0.1859, -0.3868, -0.6936, -0.0299,\n",
      "          -0.4795, -0.2998, -0.3795,  0.4457,  0.4713, -0.0185, -0.1110,\n",
      "          -0.0366,  0.2109,  0.0369, -0.4674, -0.2995,  0.2409,  0.1765]]],\n",
      "       grad_fn=<ConvolutionBackward0>)\n",
      "tensor([[[-0.0446, -0.1115, -0.0431, -0.0711, -0.1146, -0.1058, -0.0979,\n",
      "          -0.1205, -0.1049, -0.1261, -0.1084, -0.1138, -0.1679, -0.1088,\n",
      "          -0.0636, -0.0761, -0.1908,  0.0334, -0.0152, -0.1071, -0.1122,\n",
      "          -0.1065, -0.0984, -0.1044, -0.0904, -0.1380, -0.0618, -0.1113]]],\n",
      "       grad_fn=<ConvolutionBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch, torch.nn as nn\n",
    "from typing import Optional\n",
    "import sys; sys.path.append(\"..\")\n",
    "import torchbend as tb\n",
    "\n",
    "class Greg(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv_module_1 = nn.Conv1d(1, 4, 3)\n",
    "        self.conv_module_2 = nn.Conv1d(4, 1, 3)\n",
    "    def forward(self, x):\n",
    "        out_1 = self.conv_module_1(x)\n",
    "        out_2 = self.conv_module_2(out_1)\n",
    "        return out_2\n",
    "    \n",
    "class Square(tb.BendingCallback):\n",
    "    # define class attributes to provide meta information on what the callback can do\n",
    "    weight_compatible = True \n",
    "    activation_compatible = True\n",
    "    jit_compatible = True \n",
    "    nntilde_compatible = True \n",
    "\n",
    "    def apply_to_param(self, idx: int, param: nn.Parameter, cache: Optional[torch.Tensor]):\n",
    "        \"\"\"\n",
    "        for weight bending, apply_to_param receives three arguments : \n",
    "        - idx of recorded param (int) : bended parameters are recorded as lists to be jit-compatible, such that providing the id is useful to recover \n",
    "        paramter-wise buffers\n",
    "        - param : the module parameter, modified in place\n",
    "        - cache : (optional): callbacks cache the original parameter value for dynamical modification\n",
    "        \"\"\"\n",
    "        if cache is not None:\n",
    "            param.set_(cache * cache)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, name: Optional[str] = None):\n",
    "        \"\"\"\n",
    "        for activation bending, apply_to_param receives two arguments : \n",
    "        - x : value of activation\n",
    "        - name : the activation name, that can be used to retrieve activation-wise buffers\n",
    "        \"\"\"\n",
    "        return x * x\n",
    "\n",
    "\n",
    "module = Greg()\n",
    "bended = tb.BendedModule(module)\n",
    "x = torch.randn(1, 1, 32)\n",
    "_, out = bended.trace(x=x, _return_out=True)\n",
    "print(out[0])\n",
    "\n",
    "bended.bend(Square(), \"conv_module_1$\", \"conv_module.1.weight\")\n",
    "out = bended(x)\n",
    "print(out)\n",
    "\n",
    "scripted = bended.script()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stateful callbacks \n",
    "\n",
    "Stateful callbacks may be a little trickier, nothing heavy do not worry but some steps can be cumbersome due to torch.jit. Stateful callbacks implies overriding some more methods :\n",
    "- `register_parameter` : register a bended parameter in the callback module\n",
    "- `register_activation` : register a bended activation shape in the callback module\n",
    "- `update`, that updates inner states after a `BendingParameter` change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.3871,  0.3391, -0.0650,  0.0677,  0.4748, -0.0197, -0.0200,\n",
      "           0.4790,  0.6838, -0.0125, -0.1180,  0.0709,  0.4235,  0.3217,\n",
      "           0.3106,  0.1544,  0.1873,  0.1464, -0.4558,  0.0875,  0.7703,\n",
      "           0.3424, -0.0955,  0.1810,  0.6059,  0.0449,  0.0875,  0.5416]]],\n",
      "       grad_fn=<ConvolutionBackward0>)\n",
      "tensor([[[ 0.3871,  0.3391, -0.0650,  0.0677,  0.4748, -0.0197, -0.0200,\n",
      "           0.4790,  0.6838, -0.0125, -0.1180,  0.0709,  0.4235,  0.3217,\n",
      "           0.3106,  0.1544,  0.1873,  0.1464, -0.4558,  0.0875,  0.7703,\n",
      "           0.3424, -0.0955,  0.1810,  0.6059,  0.0449,  0.0875,  0.5416]]],\n",
      "       grad_fn=<MulBackward0>)\n",
      "tensor([[[ 0.0000,  0.0000, -0.1217, -0.0214,  0.0000,  0.0000, -0.1343,\n",
      "           0.0000,  0.0000,  0.0000, -0.3166, -0.0575,  0.0000,  0.0000,\n",
      "           0.0000,  0.0000,  0.0000,  0.0000, -0.6491, -0.0991,  0.0000,\n",
      "           0.0000, -0.1503,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]]],\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "class UpClamp(tb.BendingCallback):\n",
    "    weight_compatible = True \n",
    "    activation_compatible = True\n",
    "    jit_compatible = True \n",
    "    nntilde_compatible = True \n",
    "\n",
    "    # provide this to inform BendingCallback that this init arg can be contorlled\n",
    "    controllable_params = ['threshold']\n",
    "\n",
    "    def __init__(self, threshold = 1.):\n",
    "        super().__init__()\n",
    "        self.threshold = threshold\n",
    "        self._masks = torch.nn.ParameterList()\n",
    "        self._mask_names = []\n",
    "\n",
    "    def _add_mask(self, name, parameter):\n",
    "        mask = nn.Parameter((parameter.data > self.get('threshold')).float(), requires_grad=False)\n",
    "        self._masks.append(mask)\n",
    "        # disable gradient\n",
    "        self._mask_names.append(name)\n",
    "\n",
    "    def register_parameter(self, parameter, name=None, cache = True):\n",
    "        name = super().register_parameter(parameter, name=name, cache=cache)\n",
    "        self._add_mask(name, parameter=parameter)\n",
    "\n",
    "    def register_activation(self, name, shape):\n",
    "        name = super().register_activation(name, shape)\n",
    "        # here we don't need to do anything, as only parameter updates require states\n",
    "        return name\n",
    "    \n",
    "    def get_mask_from_index(self, idx: int):\n",
    "        # Don't judge me, this is because torch.jit only allows literal indexing. \n",
    "        for i, v in enumerate(self._masks):\n",
    "            if i == idx:\n",
    "                return v\n",
    "        raise tb.BendingCallbackException('%d not present in masks'%idx)\n",
    "    \n",
    "    def update(self):\n",
    "        for i, v in enumerate(self._masks):\n",
    "                v.data.set_((self.get_cache(i) > self.get('threshold')).float())\n",
    "\n",
    "    def apply_to_param(self, idx: int, param: nn.Parameter, cache: Optional[torch.Tensor]):\n",
    "        \"\"\"\n",
    "        for weight bending, apply_to_param receives three arguments : \n",
    "        - idx of recorded param (int) : bended parameters are recorded as lists to be jit-compatible, such that providing the id is useful to recover \n",
    "        paramter-wise buffers\n",
    "        - param : the module parameter, modified in place\n",
    "        - cache : (optional): callbacks cache the original parameter value for dynamical modification\n",
    "        \"\"\"\n",
    "        assert cache is not None\n",
    "        param.set_(cache * self.get_mask_from_index(idx))\n",
    "\n",
    "    def forward(self, x: torch.Tensor, name: Optional[str] = None):\n",
    "        \"\"\"\n",
    "        for activation bending, apply_to_param receives two arguments : \n",
    "        - x : value of activation\n",
    "        - name : the activation name, that can be used to retrieve activation-wise buffers\n",
    "        \"\"\"\n",
    "        return x * (x < self.get('threshold')).float()\n",
    "\n",
    "module = Greg()\n",
    "bended = tb.BendedModule(module)\n",
    "x = torch.randn(1, 1, 32)\n",
    "_, out = bended.trace(x=x, _return_out=True)\n",
    "print(out[0])\n",
    "\n",
    "c1 = tb.BendingParameter('threshold', value=1.)\n",
    "bended.bend(UpClamp(threshold=c1), \"conv_module_2$\", \"conv_module_1.weight\")\n",
    "out = bended(x)\n",
    "print(out)\n",
    "\n",
    "bended.update(c1.name, 0.)\n",
    "out = bended(x)\n",
    "print(out)\n",
    "\n",
    "scripted = bended.script()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
