{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining callbacks\n",
    "\n",
    "\n",
    "We will see in this tutorial how to define a bending callaback to perform custom callback operations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining stateless callbacks\n",
    "\n",
    "To start easy, let's define a callback that does not need any internal buffer. A bending callback must define two functions : \n",
    "- the `forward` function, that is used for activation bending\n",
    "- the `apply_to_param` function, that is used for weight bending\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.4892,  0.5153,  0.4182,  0.4789,  0.3084,  0.2741,  0.1425,\n",
      "           0.1976,  0.0509,  0.1303,  0.0505,  0.2202,  0.1965,  0.2649,\n",
      "           0.2580,  0.4473,  0.2939,  0.2936,  0.1679, -0.0621,  0.3148,\n",
      "           0.1342,  0.3729,  0.3753,  0.1876,  0.2506, -0.1649, -0.0830]]],\n",
      "       grad_fn=<ConvolutionBackward0>)\n",
      "tensor([[[-0.1816, -0.1793, -0.1797, -0.1860, -0.1238, -0.1114, -0.0931,\n",
      "          -0.0999, -0.0282, -0.0395, -0.0591, -0.0896, -0.0855, -0.1226,\n",
      "          -0.1002, -0.1346, -0.1343, -0.1034, -0.0558, -0.0400, -0.0925,\n",
      "          -0.0800, -0.1440, -0.1201, -0.0507, -0.0346, -0.0106, -0.0324]]],\n",
      "       grad_fn=<ConvolutionBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch, torch.nn as nn\n",
    "from typing import Optional\n",
    "import sys; sys.path.append(\"..\")\n",
    "import torchbend as tb\n",
    "\n",
    "class Greg(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv_module_1 = nn.Conv1d(1, 4, 3)\n",
    "        self.conv_module_2 = nn.Conv1d(4, 1, 3)\n",
    "    def forward(self, x):\n",
    "        out_1 = self.conv_module_1(x)\n",
    "        out_2 = self.conv_module_2(out_1)\n",
    "        return out_2\n",
    "    \n",
    "class Square(tb.BendingCallback):\n",
    "    # define class attributes to provide meta information on what the callback can do\n",
    "    weight_compatible = True \n",
    "    activation_compatible = True\n",
    "    jit_compatible = True \n",
    "    nntilde_compatible = True \n",
    "\n",
    "    def apply_to_param(self, idx: int, param: nn.Parameter, cache: Optional[torch.Tensor]):\n",
    "        \"\"\"\n",
    "        for weight bending, apply_to_param receives three arguments : \n",
    "        - idx of recorded param (int) : bended parameters are recorded as lists to be jit-compatible, such that providing the id is useful to recover \n",
    "        paramter-wise buffers\n",
    "        - param : the module parameter, modified in place\n",
    "        - cache : (optional): callbacks cache the original parameter value for dynamical modification\n",
    "        \"\"\"\n",
    "        param.set_(cache * cache)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, name: Optional[str] = None):\n",
    "        \"\"\"\n",
    "        for activation bending, apply_to_param receives two arguments : \n",
    "        - x : value of activation\n",
    "        - name : the activation name, that can be used to retrieve activation-wise buffers\n",
    "        \"\"\"\n",
    "        return x * x\n",
    "\n",
    "\n",
    "module = Greg()\n",
    "bended = tb.BendedModule(module)\n",
    "x = torch.randn(1, 1, 32)\n",
    "_, out = bended.trace(x=x, _return_out=True)\n",
    "print(out[0])\n",
    "\n",
    "bended.bend(Square(), \"conv_module_1$\", \"conv_module.1.weight\")\n",
    "out = bended(x)\n",
    "print(out)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stateful callbacks \n",
    "\n",
    "Stateful callbacks may be a little trickier, nothing heavy do not worry but some steps can be cumbersome due to torch.jit.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UpClamp(tb.BendingCallback):\n",
    "    weight_compatible = True \n",
    "    activation_compatible = True\n",
    "    jit_compatible = True \n",
    "    nntilde_compatible = True \n",
    "    # provide this to inform BendingCallback that this init arg can be contorlled\n",
    "    controllable_params = ['threshold']\n",
    "\n",
    "    def __init__(self, threshold = 1.):\n",
    "        super().__init__()\n",
    "        self.threshold = threshold\n",
    "\n",
    "    def apply_to_param(self, idx: int, param: nn.Parameter, cache: Optional[torch.Tensor]):\n",
    "        \"\"\"\n",
    "        for weight bending, apply_to_param receives three arguments : \n",
    "        - idx of recorded param (int) : bended parameters are recorded as lists to be jit-compatible, such that providing the id is useful to recover \n",
    "        paramter-wise buffers\n",
    "        - param : the module parameter, modified in place\n",
    "        - cache : (optional): callbacks cache the original parameter value for dynamical modification\n",
    "        \"\"\"\n",
    "        param.set_(cache * cache)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, name: Optional[str] = None):\n",
    "        \"\"\"\n",
    "        for activation bending, apply_to_param receives two arguments : \n",
    "        - x : value of activation\n",
    "        - name : the activation name, that can be used to retrieve activation-wise buffers\n",
    "        \"\"\"\n",
    "        return x * x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
